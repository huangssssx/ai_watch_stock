# å±±è°·ç‹™å‡»é€‰è‚¡ç­–ç•¥ - å­¦æœ¯é©±åŠ¨ä¼˜åŒ–ç‰ˆ
# åŸºäº Bry-Boschan åŠ¨æ€çª—å£ã€å¸‚å€¼åˆ†å±‚æµåŠ¨æ€§ã€STH-CB æˆæœ¬æ¨¡å‹ä¼˜åŒ–
#
# ã€å¦‚ä½•ä½¿ç”¨ã€‘
# 1) Web ç«¯ï¼šè¿›å…¥â€œé€‰è‚¡â€é¡µé¢ â†’ é€‰æ‹©â€œå±±è°·ç‹™å‡»é€‰è‚¡â€ â†’ ç‚¹å‡» â€œRun Nowâ€ è¿è¡Œã€‚
#    - ç³»ç»Ÿæ‰§è¡Œæ–¹å¼ï¼šä¼šç›´æ¥æ‰§è¡Œæ•°æ®åº“é‡Œè¯¥ç­–ç•¥çš„è„šæœ¬æ–‡æœ¬ï¼ˆè¦æ±‚è„šæœ¬æœ€ç»ˆäº§å‡º `df` æˆ– `result`ï¼‰ã€‚
# 2) è„šæœ¬æ›´æ–°å…¥åº“ï¼šä¿®æ”¹æœ¬æ–‡ä»¶åï¼Œä½¿ç”¨ä¸‹é¢å‘½ä»¤å°†æœ€æ–°è„šæœ¬æ–‡æœ¬å†™å›æ•°æ®åº“ï¼š
#    - `python3 backend/scripts/insert_valley_script.py --file backend/scripts/é€‰è‚¡ç­–ç•¥/å±±è°·ç‹™å‡»é€‰è‚¡ç­–ç•¥_optimized.py --id 5 --force`
#    - å…¶ä¸­ `--id 5` æ˜¯å½“å‰åº“é‡Œâ€œå±±è°·ç‹™å‡»é€‰è‚¡â€çš„ `stock_screeners.id`ï¼ˆå¦‚ä½ åº“é‡Œ ID ä¸åŒï¼Œè¯·ä»¥å®é™…ä¸ºå‡†ï¼‰ã€‚
#
# ã€è¾“å‡ºçº¦å®šï¼ˆå¿…é¡»ï¼‰ã€‘
# - ä½ éœ€è¦åœ¨è„šæœ¬æœ«å°¾å®šä¹‰ï¼š
#   - `df`: pandas.DataFrameï¼ˆæ¨èï¼‰ã€‚ç³»ç»Ÿä¼šæŠŠå®ƒè½¬æˆ JSON åˆ—è¡¨å±•ç¤ºä¸è½åº“ã€‚
#   - æˆ– `result`: List[Dict]ï¼ˆå¯é€‰ï¼‰ã€‚
# - å»ºè®®åˆ—åè‡³å°‘åŒ…å«ï¼š`ä»£ç `ã€`åç§°`ã€`æœ€æ–°ä»·`ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºä¸â€œä¸€é”®åŠ å…¥è‡ªé€‰â€è¯†åˆ«ï¼‰ã€‚
#
# ã€ä»·æ ¼é˜ˆå€¼è¿‡æ»¤ï¼ˆç»‡å¸ƒæœº / Price Thresholdï¼‰ã€‘
# - A è‚¡æœ€å°å˜åŠ¨å•ä½ï¼ˆTickï¼‰å›ºå®šä¸º 0.01 å…ƒï¼Œè‚¡ä»·è¶Šä½ Tick å æ¯”è¶Šå¤§ï¼Œæ›²çº¿æ›´â€œé”¯é½¿â€ï¼š
#   - Tick Impact = 0.01 / Price
# - æœ¬è„šæœ¬æä¾›å¯å¼€å…³çš„â€œä½ä»·è¿‡æ»¤â€ï¼Œé»˜è®¤å‰”é™¤ `æœ€æ–°ä»· < 10.0`ï¼š
#   - `PRICE_THRESHOLD_ENABLED`ï¼šæ˜¯å¦å¯ç”¨
#   - `PRICE_THRESHOLD_MIN_PRICE`ï¼šæœ€ä½ä»·é˜ˆå€¼ï¼ˆæ¿€è¿›å¯è®¾ 5.0ï¼‰
#
# ã€è¿è¡Œæ³¨æ„äº‹é¡¹ã€‘
# - æœ¬è„šæœ¬ä¼šæ‹‰å–å…¨å¸‚åœºå¿«ç…§ + å¤šåªè‚¡ç¥¨çš„å†å²æ•°æ®ï¼Œè¿è¡Œæ—¶é—´ä¸å€™é€‰æ± å¤§å°ã€ç½‘ç»œè´¨é‡å¼ºç›¸å…³ã€‚
# - akshare æ•°æ®æ¥å£æœ‰æ—¶ä¼šæŠ–åŠ¨/é™æµï¼Œå‡ºç°å¼‚å¸¸æ—¶ä¼šè·³è¿‡ä¸ªè‚¡æˆ–è¿”å›ç©ºç»“æœï¼Œè¿™æ˜¯æ­£å¸¸ç°è±¡ã€‚
# - ä¾èµ–ï¼š`akshare`ã€`pandas`ã€`numpy`ã€`talib`ã€`scipy`ï¼ˆç¼ºä¾èµ–ä¼šå¯¼è‡´è¿è¡Œå¤±è´¥ï¼‰ã€‚

import akshare as ak
import pandas as pd
import numpy as np
import datetime
import talib
from scipy.signal import argrelextrema

# --- å‚æ•°é…ç½® ---
RECENT_VOLUME_DAYS = 5
VOLUME_BASE_DAYS = 120

# å¸‚å€¼åˆ†å±‚é˜ˆå€¼ (å•ä½: å…ƒ)
CAP_SMALL = 100 * 1e8
CAP_LARGE = 500 * 1e8

# ç¼©é‡é˜ˆå€¼ (åŠ¨æ€è°ƒæ•´)
VOL_RANK_LARGE = 0.25
VOL_RANK_MID = 0.15
VOL_RANK_SMALL = 0.10

# åŸºç¡€è¿‡æ»¤
MIN_TURNOVER_AMOUNT = 30000000
MAX_PRICE_CHANGE = 6.0
PRICE_THRESHOLD_ENABLED = True
PRICE_THRESHOLD_MIN_PRICE = 10.0

# è¯„åˆ†é—¨æ§›
THRESHOLD_HIGH_QUALITY = 7
THRESHOLD_POTENTIAL = 4

AR_SPREAD_WINDOW = 20
AR_SPREAD_LOOKBACK = 120
RSV_WINDOW = 20
RSV_LOOKBACK = 120
ILLIQ_COMPOSITE_THRESHOLD = 0.70
AR_SPREAD_RANK_SKIP = 0.90

# BBç®—æ³•å‚æ•°
BB_WINDOW = 5  # æœ€å°ç›¸ä½é•¿åº¦

# STH-CB å‚æ•°
OVERHEAD_VOL_WINDOW = 20
DRAWDOWN_THRESHOLD = 0.20

# è¯„åˆ†æ ‡å‡†
SCORE_CRITERIA = {
    "volume_extreme": 3,
    "volume_high": 1,
    "macd_div": 3,
    "rsi_div": 2,
    "illiq_composite": 2,
    "ofi_confirm": 1,
    "vrp_signal": 2,
    "rebound_confirm": 2,
    "sector_fund_flow_strong": 2,
    "sector_fund_flow_ok": 1,
    "weibo_panic": 1,
    "heat_penalty": 1,
    "weibo_hype_penalty": 1,
}

# --- æ ¸å¿ƒå‡½æ•° ---

def _normalize_symbol(code: str) -> str:
    s = "" if code is None else str(code).strip()
    if len(s) >= 8 and s[:2].lower() in ("sh", "sz", "bj"):
        return s[2:]
    return s

def _kalman_filter_1d(values: pd.Series, q: float = 1e-5, r_scale: float = 0.20):
    """å¡å°”æ›¼æ»¤æ³¢é™å™ªå¤„ç†"""
    v = pd.to_numeric(values, errors="coerce").to_numpy(dtype=float)
    if v.size == 0:
        return values
    first_finite_idx = int(np.argmax(np.isfinite(v))) if np.isfinite(v).any() else None
    if first_finite_idx is None:
        return values
    dv = np.diff(v)
    dv = dv[np.isfinite(dv)]
    base_var = float(np.nanvar(dv)) if dv.size else 0.0
    r = max(1e-9, r_scale * base_var)
    x = float(v[first_finite_idx])
    p = 1.0
    out = np.empty_like(v, dtype=float)
    for i in range(v.size):
        p = p + q
        if np.isfinite(v[i]):
            k = p / (p + r)
            x = x + k * (v[i] - x)
            p = (1.0 - k) * p
        out[i] = x
    return pd.Series(out, index=values.index)

def _get_bb_troughs(series: pd.Series, window: int = BB_WINDOW):
    """
    åŸºäºBBè§„åˆ™çš„ç»“æ„åŒ–ä½ç‚¹è¯†åˆ« (Rao & Rojas 2025)
    """
    data = series.values
    # å¯»æ‰¾å±€éƒ¨æå°å€¼
    local_mins_tuple = argrelextrema(data, np.less, order=window)
    local_mins = local_mins_tuple[0]
    
    refined_troughs = []
    if len(local_mins) > 0:
        refined_troughs.append(local_mins[0])
        for i in range(1, len(local_mins)):
            # ç¡®ä¿ä½ç‚¹é—´éš”
            if local_mins[i] - refined_troughs[-1] >= window:
                refined_troughs.append(local_mins[i])
            else:
                # å¦‚æœé—´éš”å¤ªè¿‘ï¼Œä¿ç•™æ›´ä½çš„é‚£ä¸ª
                if data[local_mins[i]] < data[refined_troughs[-1]]:
                    refined_troughs[-1] = local_mins[i]
                    
    return refined_troughs

def detect_dynamic_divergence(smooth_p: pd.Series, indicator: pd.Series):
    """
    åŠ¨æ€çª—å£èƒŒç¦»æ£€æµ‹ (Nowcasting)
    """
    if len(smooth_p) < 60: return False
    
    troughs = _get_bb_troughs(smooth_p)
    if len(troughs) < 2: return False
    
    # é”å®šæœ€è¿‘çš„ä¸¤ä¸ªç»“æ„åŒ–ä½ç‚¹
    last_idx = troughs[-1]
    prev_idx = troughs[-2]
    
    # å¦‚æœæœ€è¿‘çš„ä½ç‚¹ç¦»ç°åœ¨å¤ªè¿œ(è¶…è¿‡15å¤©)ï¼Œåˆ™ä¿¡å·å¤±æ•ˆ
    if (len(smooth_p) - 1) - last_idx > 15:
        return False
    
    p_last, p_prev = smooth_p.iloc[last_idx], smooth_p.iloc[prev_idx]
    i_last, i_prev = indicator.iloc[last_idx], indicator.iloc[prev_idx]
    
    # ç‰›èƒŒç¦»é€»è¾‘ï¼šä»·æ ¼åˆ›æ–°ä½ï¼ˆæˆ–äºŒæ¬¡æ¢åº•ï¼‰ï¼ŒæŒ‡æ ‡æ˜¾è‘—æŠ¬å‡
    if p_last <= p_prev * 1.02 and i_last > i_prev * 1.05:
        # å¢åŠ åŠ é€Ÿåº¦éªŒè¯: è·Œé€Ÿéœ€æ”¾ç¼“ (äºŒé˜¶å·®åˆ† > 0)
        recent_acceleration = (smooth_p.diff().diff()).iloc[last_idx]
        if recent_acceleration > 0:
            return True
    return False

def calc_composite_illiq(close: pd.Series, amount: pd.Series, high: pd.Series, low: pd.Series):
    """
    Amihud-HL-FHTå¤åˆæµåŠ¨æ€§æŒ‡æ ‡ (Dong et al. 2024)
    """
    if len(close) < 20: return 0, 0
    
    # 1. Amihud (|Ret| / Amt)
    rets = close.pct_change().abs()
    amihud = rets / (amount + 1e-9) * 1e8
    
    # 2. HL Spread (Corwin-Schultz ç®€åŒ–ç‰ˆ)
    hl_ratio = (high - low) / (close + 1e-9)
    
    # è®¡ç®—æœ€è¿‘20æ—¥çš„å¹³å‡å€¼ä½œä¸ºå½“å‰å€¼
    curr_amihud = amihud.iloc[-20:].mean()
    curr_hl = hl_ratio.iloc[-20:].mean()
    
    # è®¡ç®—å†å²åˆ†ä½ (è¿‡å»120å¤©)
    hist_amihud = amihud.iloc[-120:]
    hist_hl = hl_ratio.iloc[-120:]
    
    amihud_rank = (hist_amihud <= curr_amihud).mean()
    hl_rank = (hist_hl <= curr_hl).mean()

    composite = (amihud_rank + hl_rank) / 2.0
    return composite

def calc_ar_spread_rank(high: pd.Series, low: pd.Series, close: pd.Series):
    h = pd.to_numeric(high, errors="coerce")
    l = pd.to_numeric(low, errors="coerce")
    c = pd.to_numeric(close, errors="coerce")
    h = np.log(h.where(h > 0))
    l = np.log(l.where(l > 0))
    c = np.log(c.where(c > 0))
    eta = (h + l) / 2.0
    term = 4.0 * (c - eta) * (c.shift(1) - eta.shift(1))
    ar = np.sqrt(np.maximum(term, 0.0))
    ar_roll = ar.rolling(window=AR_SPREAD_WINDOW, min_periods=max(3, AR_SPREAD_WINDOW // 3)).mean()
    curr = ar_roll.iloc[-1] if len(ar_roll) else np.nan
    hist = ar_roll.iloc[-AR_SPREAD_LOOKBACK:].dropna()
    rank = float((hist <= curr).mean()) if len(hist) and pd.notna(curr) else np.nan
    return rank, float(curr) if pd.notna(curr) else np.nan

def calculate_downside_rsv_rank(close: pd.Series):
    c = pd.to_numeric(close, errors="coerce").where(lambda x: x > 0)
    r = c.pct_change()
    down = np.minimum(r, 0.0) ** 2
    tot = r ** 2
    down_sum = down.rolling(window=RSV_WINDOW, min_periods=max(3, RSV_WINDOW // 3)).sum()
    tot_sum = tot.rolling(window=RSV_WINDOW, min_periods=max(3, RSV_WINDOW // 3)).sum()
    ratio = down_sum / (tot_sum + 1e-9)
    ratio = ratio.replace([np.inf, -np.inf], np.nan)
    last = ratio.iloc[-1] if len(ratio) else np.nan
    hist = ratio.iloc[-RSV_LOOKBACK:].dropna()
    rank = float((hist <= last).mean()) if len(hist) and pd.notna(last) else np.nan
    return rank, float(last) if pd.notna(last) else np.nan

def check_overhead_supply(close: pd.Series, volume: pd.Series, amount: pd.Series, current_price: float):
    """
    åŠå±±è…°è§„é¿æ¨¡å— (STH-CB & Drawdown)
    """
    if len(close) < 252: return False # éœ€è¦ä¸€å¹´æ•°æ®è®¡ç®— drawdown
    
    # STH-CB: 20æ—¥ VWAP
    vol_20 = volume.rolling(20).sum()
    amt_20 = amount.rolling(20).sum()
    typical_px = float(pd.to_numeric(close, errors="coerce").tail(60).median())
    typical_amt_per_vol = float((pd.to_numeric(amount, errors="coerce") / (pd.to_numeric(volume, errors="coerce") + 1e-9)).tail(60).median())
    vol_unit = 100.0 if (np.isfinite(typical_px) and typical_px > 0 and np.isfinite(typical_amt_per_vol) and typical_amt_per_vol > typical_px * 20.0) else 1.0
    vwap_20 = amt_20 / (vol_20 * vol_unit + 1e-9)
    
    current_vwap = vwap_20.iloc[-1]
    prev_vwap = vwap_20.iloc[-2]
    
    # è§„åˆ™1: ä»·æ ¼ > VWAP æˆ– VWAP æ‹å¤´å‘ä¸Š
    vwap_slope = current_vwap - prev_vwap
    is_above_cost = (current_price > current_vwap) or (vwap_slope > 0)
    
    # è§„åˆ™2: è·ç¦»52å‘¨é«˜ç‚¹éœ€æœ‰è¶³å¤Ÿæ·±åº¦ (>20%)
    high_52w = close.rolling(252).max().iloc[-1]
    drawdown = (high_52w - current_price) / high_52w
    is_deep_enough = drawdown > DRAWDOWN_THRESHOLD
    
    return is_above_cost and is_deep_enough

def dynamic_volume_score(volume: pd.Series, mkt_cap: float):
    """
    å¸‚å€¼åˆ†å±‚åŠ¨æ€ç¼©é‡è¯„åˆ†
    """
    if len(volume) < 120: return 0, 0
    
    # å®šä¹‰åŠ¨æ€é˜ˆå€¼
    if mkt_cap > CAP_LARGE:
        threshold = VOL_RANK_LARGE
    elif mkt_cap < CAP_SMALL:
        threshold = VOL_RANK_SMALL
    else:
        threshold = VOL_RANK_MID
        
    # è®¡ç®—120æ—¥ç¼©é‡æ’å
    curr_vol = volume.iloc[-RECENT_VOLUME_DAYS:].median()
    hist_vol = volume.iloc[-VOLUME_BASE_DAYS:]
    vol_rank = (hist_vol <= curr_vol).mean()
    
    if vol_rank < threshold:
        return SCORE_CRITERIA["volume_extreme"], vol_rank
    elif vol_rank < 0.40:
        return SCORE_CRITERIA["volume_high"], vol_rank
    return 0, vol_rank

def calculate_vrp_score(close: pd.Series):
    """
    æ³¢åŠ¨ç‡é£é™©æº¢ä»· (VRP)
    """
    if len(close) < 20: return 0, 0
    
    rets = close.pct_change()
    rv = rets.rolling(5).std() # Realized Volatility
    iv_proxy = rets.rolling(20).std() # Implied Volatility Proxy (ç”¨é•¿æœŸæ³¢åŠ¨ç‡ä»£æ›¿)
    
    vrp = iv_proxy - rv
    
    # è®¡ç®—VRPåˆ†ä½
    curr_vrp = vrp.iloc[-1]
    hist_vrp = vrp.iloc[-120:]
    vrp_rank = (hist_vrp <= curr_vrp).mean()
    
    if vrp_rank > 0.8:
        return SCORE_CRITERIA["vrp_signal"], vrp_rank
    return 0, vrp_rank

def calculate_ofi_signal(open_s, close_s, vol_s):
    # ç®€åŒ–çš„ OFI é€»è¾‘
    o = pd.to_numeric(open_s, errors="coerce")
    c = pd.to_numeric(close_s, errors="coerce")
    v = pd.to_numeric(vol_s, errors="coerce").fillna(0.0)
    diff = c - o
    sgn = np.sign(diff)
    ofi = sgn * v
    ofi_sum = ofi.rolling(10).sum()
    vol_sum = v.rolling(10).sum()
    ratio = ofi_sum / (vol_sum.abs() + 1e-9)
    last_ratio = ratio.iloc[-1]
    if last_ratio > 0.1:
        return SCORE_CRITERIA["ofi_confirm"], last_ratio
    return 0, last_ratio

# --- ä¸»ç¨‹åº ---
print("ğŸ¯ ã€å±±è°·ç‹™å‡»é€‰è‚¡ç­–ç•¥ (å­¦æœ¯ä¼˜åŒ–ç‰ˆ)ã€‘å¯åŠ¨")
print("ğŸ“¡ æ­£åœ¨è·å–Aè‚¡å®æ—¶è¡Œæƒ…...")

try:
    df_market = ak.stock_zh_a_spot_em()
except Exception as e:
    print(f"âŒ è·å–è¡Œæƒ…å¤±è´¥: {e}")
    df_market = pd.DataFrame()

if not df_market.empty:
    # é¢„è¿‡æ»¤
    df_market = df_market[~df_market["åç§°"].str.contains("ST|é€€", na=False)]
    df_market = df_market[abs(df_market["æ¶¨è·Œå¹…"]) <= MAX_PRICE_CHANGE]
    if "æˆäº¤é¢" in df_market.columns:
        df_market = df_market[df_market["æˆäº¤é¢"] >= MIN_TURNOVER_AMOUNT]
    if PRICE_THRESHOLD_ENABLED and "æœ€æ–°ä»·" in df_market.columns:
        df_market["æœ€æ–°ä»·"] = pd.to_numeric(df_market["æœ€æ–°ä»·"], errors="coerce")
        df_market = df_market.dropna(subset=["æœ€æ–°ä»·"])
        df_market = df_market[df_market["æœ€æ–°ä»·"] >= PRICE_THRESHOLD_MIN_PRICE]
    
    if len(df_market) > 300:
        df_market = df_market.sort_values(by="æ¢æ‰‹ç‡", ascending=True).head(300)

    sector_fund_flow_map = {}
    try:
        fund_flow_df = ak.stock_sector_fund_flow_rank(indicator="5æ—¥", sector_type="è¡Œä¸šèµ„é‡‘æµ")
        if fund_flow_df is not None and not fund_flow_df.empty and "åç§°" in fund_flow_df.columns:
            col = "5æ—¥ä¸»åŠ›å‡€æµå…¥-å‡€å æ¯”"
            if col in fund_flow_df.columns:
                ff = fund_flow_df[["åç§°", col]].copy()
                ff["åç§°"] = ff["åç§°"].astype(str)
                ff[col] = pd.to_numeric(ff[col], errors="coerce")
                ff = ff.dropna(subset=["åç§°", col])
                sector_fund_flow_map = {str(r["åç§°"]): float(r[col]) for _, r in ff.iterrows()}
    except Exception:
        sector_fund_flow_map = {}

    hot_rank_map = {}
    try:
        hot_df = ak.stock_hot_rank_em()
        if hot_df is not None and not hot_df.empty and "ä»£ç " in hot_df.columns and "å½“å‰æ’å" in hot_df.columns:
            hd = hot_df[["ä»£ç ", "å½“å‰æ’å"]].copy()
            hd["ä»£ç "] = hd["ä»£ç "].astype(str).map(_normalize_symbol)
            hd["å½“å‰æ’å"] = pd.to_numeric(hd["å½“å‰æ’å"], errors="coerce")
            hd = hd.dropna(subset=["ä»£ç ", "å½“å‰æ’å"])
            hot_rank_map = {str(r["ä»£ç "]): int(r["å½“å‰æ’å"]) for _, r in hd.iterrows()}
    except Exception:
        hot_rank_map = {}

    weibo_rate_map = {}
    try:
        weibo_df = ak.stock_js_weibo_report(time_period="CNHOUR24")
        if weibo_df is not None and not weibo_df.empty and "name" in weibo_df.columns and "rate" in weibo_df.columns:
            wb = weibo_df[["name", "rate"]].copy()
            wb["name"] = wb["name"].astype(str)
            wb["rate"] = pd.to_numeric(wb["rate"], errors="coerce")
            wb = wb.dropna(subset=["name", "rate"])
            weibo_rate_map = {str(r["name"]): float(r["rate"]) for _, r in wb.iterrows()}
    except Exception:
        weibo_rate_map = {}

    industry_cache = {}

    def _get_industry(symbol: str):
        sym = _normalize_symbol(symbol)
        if sym in industry_cache:
            return industry_cache[sym]
        industry = None
        try:
            info_df = ak.stock_individual_info_em(symbol=sym)
            if info_df is not None and not info_df.empty and "item" in info_df.columns and "value" in info_df.columns:
                mask = info_df["item"].astype(str) == "è¡Œä¸š"
                if mask.any():
                    industry = str(info_df.loc[mask, "value"].iloc[0]).strip()
                    if not industry:
                        industry = None
        except Exception:
            industry = None
        industry_cache[sym] = industry
        return industry

    end_date = datetime.datetime.now()
    start_date = end_date - datetime.timedelta(days=730)
    start_date_str = start_date.strftime("%Y%m%d")
    end_date_str = end_date.strftime("%Y%m%d")
    
    results = []
    count = 0
    total = len(df_market)
    start_ts = datetime.datetime.now()
    progress_every = 10 if total <= 120 else 25
    print(f"ğŸ§® å€™é€‰æ± : {total} åªï¼Œè¿›åº¦æ­¥é•¿: {progress_every}")
    
    for _, row in df_market.iterrows():
        count += 1
        symbol = row["ä»£ç "]
        name = row["åç§°"]
        current_price = float(row["æœ€æ–°ä»·"])
        pct_chg = float(row["æ¶¨è·Œå¹…"])
        mkt_cap = float(row["æµé€šå¸‚å€¼"]) if "æµé€šå¸‚å€¼" in row and pd.notna(row["æµé€šå¸‚å€¼"]) else 100e8
        
        if count == 1 or count == total or (progress_every > 0 and count % progress_every == 0):
            elapsed = (datetime.datetime.now() - start_ts).total_seconds()
            speed = count / elapsed if elapsed > 0 else 0.0
            eta_sec = int((total - count) / speed) if speed > 0 else -1
            eta_str = f"{eta_sec}s" if eta_sec >= 0 else "?"
            print(f"â³ è¿›åº¦: {count}/{total}  ç”¨æ—¶:{elapsed:.1f}s  ETA:{eta_str}")
            
        try:
            df_hist = ak.stock_zh_a_hist(symbol=symbol, period="daily", start_date=start_date_str, end_date=end_date_str, adjust="qfq")
            if df_hist is None or df_hist.empty or len(df_hist) < 120:
                continue
                
            # æ•°æ®æ¸…æ´—
            df_hist["å¼€ç›˜"] = pd.to_numeric(df_hist["å¼€ç›˜"], errors="coerce")
            df_hist["æ”¶ç›˜"] = pd.to_numeric(df_hist["æ”¶ç›˜"], errors="coerce")
            df_hist["æœ€é«˜"] = pd.to_numeric(df_hist["æœ€é«˜"], errors="coerce")
            df_hist["æœ€ä½"] = pd.to_numeric(df_hist["æœ€ä½"], errors="coerce")
            df_hist["æˆäº¤é‡"] = pd.to_numeric(df_hist["æˆäº¤é‡"], errors="coerce")
            df_hist["æˆäº¤é¢"] = pd.to_numeric(df_hist["æˆäº¤é¢"], errors="coerce")
            
            close = df_hist["æ”¶ç›˜"]
            open_ = df_hist["å¼€ç›˜"]
            high = df_hist["æœ€é«˜"]
            low = df_hist["æœ€ä½"]
            volume = df_hist["æˆäº¤é‡"]
            amount = df_hist["æˆäº¤é¢"]
            
            # 1. åŠå±±è…°è§„é¿ (Filter)
            is_safe = check_overhead_supply(close, volume, amount, current_price)
            if not is_safe:
                continue
                
            score = 0
            signals = []
            
            # å¡å°”æ›¼å¹³æ»‘
            smooth_p = _kalman_filter_1d(close)
            
            # ç¼©é‡è¯„åˆ†
            v_score, v_rank = dynamic_volume_score(volume, mkt_cap)
            if v_score > 0:
                score += v_score
                signals.append(f"ç¼©é‡({int(v_rank*100)}%)")
                
            # VRP è¯„åˆ†
            vrp_score, vrp_rank = calculate_vrp_score(close)
            if vrp_score > 0:
                score += vrp_score
                signals.append("VRPææ…Œ")
            
            # MACD
            macd, signal, _ = talib.MACD(smooth_p.values)
            if detect_dynamic_divergence(smooth_p, pd.Series(macd)):
                score += SCORE_CRITERIA["macd_div"]
                signals.append("MACDåº•")
                
            # RSI
            rsi = talib.RSI(smooth_p.values, timeperiod=14)
            if detect_dynamic_divergence(smooth_p, pd.Series(rsi)):
                score += SCORE_CRITERIA["rsi_div"]
                signals.append("RSIåº•")
                
            # å¤åˆILLIQ
            comp_illiq = calc_composite_illiq(close, amount, high, low)
            if comp_illiq > ILLIQ_COMPOSITE_THRESHOLD: 
                 score += SCORE_CRITERIA["illiq_composite"]
                 signals.append("ILLIQå¸æ”¶")

            ar_rank, ar_spread = calc_ar_spread_rank(high, low, close)
            if pd.notna(ar_rank) and ar_rank >= AR_SPREAD_RANK_SKIP:
                continue

            down_rank, down_ratio = calculate_downside_rsv_rank(close)
            if pd.notna(down_rank) and down_rank > 0.80:
                continue
                 
            # OFI
            ofi_score, ofi_val = calculate_ofi_signal(open_, close, volume)
            if ofi_score > 0:
                score += ofi_score
                signals.append("OFI+")
                
            # å‡çº¿æ”¯æ’‘/å¯åŠ¨
            ma5 = close.rolling(5).mean()
            if len(ma5) > 2 and current_price > ma5.iloc[-1] and ma5.iloc[-1] > ma5.iloc[-2]:
                score += SCORE_CRITERIA["rebound_confirm"]
                signals.append("å¯åŠ¨")

            industry = None
            sector_ff = None
            hot_rank = None
            weibo_rate = None

            if score >= THRESHOLD_POTENTIAL - 2:
                sym_norm = _normalize_symbol(symbol)

                if hot_rank_map:
                    hot_rank = hot_rank_map.get(sym_norm)
                    if hot_rank is not None and hot_rank <= 20:
                        score -= SCORE_CRITERIA["heat_penalty"]
                        signals.append("çƒ­åº¦è¿‡é«˜")

                if weibo_rate_map:
                    weibo_rate = weibo_rate_map.get(str(name))
                    if weibo_rate is not None and np.isfinite(weibo_rate):
                        if weibo_rate <= -2.0:
                            score += SCORE_CRITERIA["weibo_panic"]
                            signals.append("èˆ†æƒ…åç©º")
                        elif weibo_rate >= 2.0:
                            score -= SCORE_CRITERIA["weibo_hype_penalty"]
                            signals.append("èˆ†æƒ…åçƒ­")

                if sector_fund_flow_map:
                    industry = _get_industry(sym_norm)
                    if industry is not None:
                        sector_ff = sector_fund_flow_map.get(industry)
                        if sector_ff is not None and np.isfinite(sector_ff):
                            if sector_ff >= 0.5:
                                score += SCORE_CRITERIA["sector_fund_flow_strong"]
                                signals.append("æ¿å—å‡€æµå…¥å¼º")
                            elif sector_ff > 0:
                                score += SCORE_CRITERIA["sector_fund_flow_ok"]
                                signals.append("æ¿å—å‡€æµå…¥")

            if score >= THRESHOLD_POTENTIAL:
                results.append({
                    "ä»£ç ": symbol,
                    "åç§°": name,
                    "ç°ä»·": current_price,
                    "æ¶¨è·Œ%": pct_chg,
                    "è¯„åˆ†": score,
                    "è¡Œä¸š": industry,
                    "æ¿å—5æ—¥ä¸»åŠ›å‡€å æ¯”": round(sector_ff, 2) if sector_ff is not None and np.isfinite(sector_ff) else None,
                    "çƒ­åº¦æ’å": int(hot_rank) if hot_rank is not None else None,
                    "å¾®åš24hçƒ­åº¦": round(weibo_rate, 2) if weibo_rate is not None and np.isfinite(weibo_rate) else None,
                    "ç¼©é‡åˆ†ä½": round(v_rank, 2),
                    "VRP": round(vrp_rank, 2),
                    "ILLIQ": round(comp_illiq, 2),
                    "ARåˆ†ä½": round(ar_rank, 2) if pd.notna(ar_rank) else None,
                    "ä¸‹è¡ŒRSV": round(down_ratio, 4) if pd.notna(down_ratio) else None,
                    "ä¸‹è¡ŒRSVåˆ†ä½": round(down_rank, 2) if pd.notna(down_rank) else None,
                    "ä¿¡å·": "+".join(signals)
                })
                
        except Exception as e:
            continue

    if results:
        df_res = pd.DataFrame(results).sort_values(by="è¯„åˆ†", ascending=False)
        print("\n" + "=" * 50)
        print(f"ğŸŒŸ ã€ä¸¥é€‰æ¦œã€‘ (è¯„åˆ†>={THRESHOLD_HIGH_QUALITY})")
        print("=" * 50)
        high_q = df_res[df_res["è¯„åˆ†"] >= THRESHOLD_HIGH_QUALITY]
        if not high_q.empty:
            print(high_q.to_string(index=False))
        else:
            print("ï¼ˆæš‚æ— ç¬¦åˆä¸¥é€‰æ ‡å‡†çš„è‚¡ç¥¨ï¼‰")
        
        print("\n" + "-" * 50)
        print(f"ğŸ‘€ ã€æ½œåŠ›æ¦œã€‘ (è¯„åˆ†>={THRESHOLD_POTENTIAL})")
        print("-" * 50)
        pot = df_res[(df_res["è¯„åˆ†"] >= THRESHOLD_POTENTIAL) & (df_res["è¯„åˆ†"] < THRESHOLD_HIGH_QUALITY)]
        if not pot.empty:
            print(pot.head(300).to_string(index=False))
        else:
            print("ï¼ˆæš‚æ— ç¬¦åˆæ½œåŠ›æ ‡å‡†çš„è‚¡ç¥¨ï¼‰")
        df = df_res
        result = results
    else:
        print("æœªå‘ç°ç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨")
        df = pd.DataFrame(columns=["ä»£ç ", "åç§°", "ç°ä»·", "æ¶¨è·Œ%", "è¯„åˆ†", "ç¼©é‡åˆ†ä½", "VRP", "ILLIQ", "ARåˆ†ä½", "ä¸‹è¡ŒRSV", "ä¸‹è¡ŒRSVåˆ†ä½", "ä¿¡å·"])
        result = []

else:
    print("è·å–è¡Œæƒ…ä¸ºç©º")
    df = pd.DataFrame()
    result = []

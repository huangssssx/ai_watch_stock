# æ•°æ®å®éªŒå®¤è„šæœ¬å¯¼å‡º

- æ¥æº DBï¼š`/Users/huangchuanjian/workspace/my_projects/ai_watch_stock/backend/stock_watch.db`
- å¯¼å‡ºè¡¨ï¼š`research_scripts`
- æ¡ç›®æ•°ï¼š`15`

## ç›®å½•
- [streamlit è¡Œä¸šèµ„é‡‘æµå‘](#streamlit-è¡Œä¸šèµ„é‡‘æµå‘)
- [å¤§ç›˜å…¨æ™¯çœ‹æ¿](#å¤§ç›˜å…¨æ™¯çœ‹æ¿)
- [æ ¹æ®è¡Œä¸šå­—æ®µè·å–è‚¡ç¥¨é›†åˆ](#æ ¹æ®è¡Œä¸šå­—æ®µè·å–è‚¡ç¥¨é›†åˆ)
- [å±±è°·ç‹™å‡»è¯„åˆ†ï¼ˆæŒ‰è‚¡ç¥¨ä»£ç ï¼‰](#å±±è°·ç‹™å‡»è¯„åˆ†-æŒ‰è‚¡ç¥¨ä»£ç )
- [è¡Œä¸šèµ„é‡‘æµå‘](#è¡Œä¸šèµ„é‡‘æµå‘)
- [å¯»æ‰¾ä½ä¼°è‚¡ç¥¨](#å¯»æ‰¾ä½ä¼°è‚¡ç¥¨)
- [PE_band](#pe_band)
- [è·å–æ¿å—æ–°é—»é›†åˆ](#è·å–æ¿å—æ–°é—»é›†åˆ)
- [ä¸ªè‚¡æ–°é—»è·å–](#ä¸ªè‚¡æ–°é—»è·å–)
- [æ‹‰å–æŒ‡å®šè‚¡ç¥¨æ–°é—»åˆ—è¡¨](#æ‹‰å–æŒ‡å®šè‚¡ç¥¨æ–°é—»åˆ—è¡¨)
- [çƒ­é—¨æ¿å—ä¸»çº¿åˆ¤æ–­](#çƒ­é—¨æ¿å—ä¸»çº¿åˆ¤æ–­)
- [è·å–å…¨å¸‚åœº pe](#è·å–å…¨å¸‚åœº-pe)
- [ä¹–ç¦»ç‡è®¡ç®—](#ä¹–ç¦»ç‡è®¡ç®—)
- [å…¬å¸å…¬å‘Šè·å–](#å…¬å¸å…¬å‘Šè·å–)
- [streamlit ä¾‹å­](#streamlit-ä¾‹å­)

## streamlit è¡Œä¸šèµ„é‡‘æµå‘

- idï¼š`12`
- is_pinnedï¼š`1`
- updated_atï¼š`2026-01-16 01:32:12`
- last_run_statusï¼š`pending`

### script_content

```python
# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import akshare as ak
import altair as alt
from datetime import datetime
import traceback
import logging
import sys
import io

# ==============================================================================
# 0. æ—¥å¿—é…ç½® (å¢å¼ºç‰ˆ)
# ==============================================================================
# åˆ›å»ºä¸€ä¸ª StringIO å¯¹è±¡æ¥æ•è·æ—¥å¿—æµï¼Œä»¥ä¾¿åœ¨ UI ä¸Šæ˜¾ç¤º
log_capture_string = io.StringIO()

# é…ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger("StockApp")
logger.setLevel(logging.INFO)

# æ¸…é™¤æ—§çš„å¤„ç†å™¨ï¼Œé˜²æ­¢ Streamlit é‡è½½å¯¼è‡´é‡å¤æ‰“å°
if logger.hasHandlers():
    logger.handlers.clear()

# 1. æ§åˆ¶å°å¤„ç†å™¨ (æ‰“å°åˆ°ç»ˆç«¯)
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(console_handler)

# 2. å­—ç¬¦ä¸²æµå¤„ç†å™¨ (ç”¨äºåœ¨ UI æ˜¾ç¤º)
stream_handler = logging.StreamHandler(log_capture_string)
stream_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(stream_handler)

def log_info(msg):
    """ç»Ÿä¸€æ—¥å¿—è®°å½•å…¥å£"""
    logger.info(msg)

def log_error(msg):
    """ç»Ÿä¸€é”™è¯¯è®°å½•å…¥å£"""
    logger.error(msg)

# ==============================================================================
# 1. é¡µé¢åŸºç¡€é…ç½®
# ==============================================================================
st.set_page_config(
    page_title="Aè‚¡è¡Œä¸šèµ„é‡‘æµå‘çœ‹æ¿ (ä¿®æ­£ç‰ˆ)",
    page_icon="ğŸ“ˆ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å…¼å®¹æ€§æ£€æŸ¥
try:
    st_version = st.__version__
    major, minor, patch = map(int, st_version.split('.')[:3])
    if major < 1 or (major == 1 and minor < 35):
        st.error(f"âš ï¸ æ£€æµ‹åˆ°æ‚¨çš„ Streamlit ç‰ˆæœ¬ ({st_version}) è¾ƒæ—§ã€‚å»ºè®®å‡çº§åˆ° 1.35.0+")
except:
    pass

# ==============================================================================
# 2. æ ¸å¿ƒé€»è¾‘åŒº - æ•°æ®è·å–ä¸å¤„ç†
# ==============================================================================
class DataManager:
    """æ•°æ®ç®¡ç†ç±»ï¼šè´Ÿè´£æ•°æ®çš„è·å–ã€æ¸…æ´—ä¸ç¼“å­˜"""
    
    @staticmethod
    def _safe_numeric(series):
        """è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨åœ°å°†å«æœ‰å•ä½(ä¸‡/äº¿)çš„å­—ç¬¦ä¸²è½¬ä¸ºæ•°å€¼"""
        def convert(x):
            if pd.isna(x) or x == "": return 0.0
            if isinstance(x, (int, float)): return float(x)
            x = str(x).replace("å…ƒ", "").replace(",", "")
            factor = 1.0
            if "ä¸‡" in x:
                factor = 10000.0
                x = x.replace("ä¸‡", "")
            elif "äº¿" in x:
                factor = 100000000.0
                x = x.replace("äº¿", "")
            try:
                return float(x) * factor
            except:
                return 0.0
        return series.apply(convert)

    @staticmethod
    @st.cache_data(ttl=300)
    def get_sector_flow_rank():
        """è·å–è¡Œä¸šèµ„é‡‘æµå‘æ’åæ•°æ®"""
        log_info("ğŸš€ [Start] å¼€å§‹è°ƒç”¨ ak.stock_sector_fund_flow_rank()...")
        try:
            with st.spinner("æ­£åœ¨ä» AkShare æ‹‰å–è¡Œä¸šæ•°æ®..."):
                df = ak.stock_sector_fund_flow_rank()
                
            if df is None or df.empty:
                log_error("âŒ [Error] æ¥å£è¿”å›æ•°æ®ä¸ºç©º (None or Empty)")
                st.warning("æ¥å£è¿”å›æ•°æ®ä¸ºç©º")
                return pd.DataFrame()

            log_info(f"âœ… [Fetch] åŸå§‹æ•°æ®è·å–æˆåŠŸï¼Œå½¢çŠ¶: {df.shape}")

            # æ•°æ®æ¸…æ´—
            df = df.dropna(how='all').drop_duplicates()
            
            # åˆ—åå…¼å®¹æ€§å¤„ç†
            col_mapping = {
                "åç§°": "è¡Œä¸šåç§°",
                "ä»Šæ—¥ä¸»åŠ›å‡€æµå…¥-å‡€é¢": "ä¸»åŠ›å‡€æµå…¥",
                "ä»Šæ—¥ä¸»åŠ›å‡€æµå…¥-å‡€å æ¯”": "ä¸»åŠ›å‡€æµå…¥-å‡€å æ¯”"
            }
            df = df.rename(columns=col_mapping)
            
            # æ£€æŸ¥åˆ—åæ˜¯å¦æ˜ å°„æˆåŠŸ
            if "è¡Œä¸šåç§°" not in df.columns:
                log_error(f"âŒ [Error] ç¼ºå°‘ 'è¡Œä¸šåç§°' åˆ—ï¼Œå½“å‰åˆ—å: {list(df.columns)}")
                return pd.DataFrame()

            # ç±»å‹è½¬æ¢
            num_cols = ["ä¸»åŠ›å‡€æµå…¥", "ä¸»åŠ›å‡€æµå…¥-å‡€å æ¯”"]
            for col in num_cols:
                if col in df.columns:
                    df[col] = DataManager._safe_numeric(df[col])
            
            # æ’åº
            df = df.sort_values(by="ä¸»åŠ›å‡€æµå…¥", ascending=False).reset_index(drop=True)
            return df

        except Exception as e:
            err_msg = traceback.format_exc()
            log_error(f"âŒ [Exception] è·å–è¡Œä¸šæ•°æ®å‘ç”Ÿå¼‚å¸¸:\n{err_msg}")
            return pd.DataFrame()

    @staticmethod
    @st.cache_data(ttl=600)
    def get_sector_details(sector_name):
        """
        è·å–æŒ‡å®šè¡Œä¸šçš„æˆåˆ†è‚¡åˆ—è¡¨
        ä½¿ç”¨ ak.stock_board_industry_cons_em æ¥å£ (ç¨³å¥)
        """
        log_info(f"ğŸš€ [Start] è·å–æ¿å—æˆåˆ†è‚¡: {sector_name}")
        try:
            # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ¥å£
            df = ak.stock_board_industry_cons_em(symbol=sector_name)
            
            if df is not None and not df.empty:
                log_info(f"âœ… [Fetch] æˆåˆ†è‚¡è·å–æˆåŠŸï¼Œè¡Œæ•°: {len(df)}")
                # ç­›é€‰æ ¸å¿ƒåˆ—
                cols_to_keep = ['ä»£ç ', 'åç§°', 'æœ€æ–°ä»·', 'æ¶¨è·Œå¹…', 'æˆäº¤é¢', 'æ¢æ‰‹ç‡', 'å¸‚ç›ˆç‡-åŠ¨æ€']
                # å…¼å®¹ä¸åŒç‰ˆæœ¬è¿”å›çš„åˆ—å
                existing_cols = [c for c in cols_to_keep if c in df.columns]
                df = df[existing_cols]
                
                # ç®€å•æ•°å€¼å¤„ç†
                if 'æˆäº¤é¢' in df.columns:
                    df['æˆäº¤é¢'] = DataManager._safe_numeric(df['æˆäº¤é¢'])
                    
                return df
            else:
                log_error(f"âŒ [Error] æ¿å— [{sector_name}] è¿”å›æ•°æ®ä¸ºç©º")
                return pd.DataFrame()
        except Exception as e:
            log_error(f"âŒ [Exception] è·å–æˆåˆ†è‚¡å¤±è´¥: {str(e)}")
            return pd.DataFrame()

# ==============================================================================
# 3. UI ç»„ä»¶åŒº
# ==============================================================================

if hasattr(st, "dialog"):
    @st.dialog("æ¿å—ä¸ªè‚¡è¯¦æƒ…", width="large")
    def show_stock_list_dialog(sector_name):
        _render_stock_list(sector_name)
else:
    def show_stock_list_dialog(sector_name):
        st.sidebar.markdown("---")
        st.sidebar.subheader(f"ğŸ“Œ {sector_name} - ä¸ªè‚¡è¯¦æƒ…")
        _render_stock_list(sector_name)

def _render_stock_list(sector_name):
    """æŠ½ç¦»çš„æ¸²æŸ“é€»è¾‘"""
    st.caption(f"å½“å‰æ¿å—ï¼š{sector_name} (æ•°æ®æº: ä¸œæ–¹è´¢å¯Œ-æ¿å—æˆä»½)")
    
    with st.spinner(f"æ­£åœ¨åŠ è½½ {sector_name} çš„è‚¡ç¥¨åˆ—è¡¨..."):
        df_stocks = DataManager.get_sector_details(sector_name)
    
    if df_stocks.empty:
        st.warning(f"âš ï¸ æœªèƒ½è·å–åˆ° [{sector_name}] çš„æˆåˆ†è‚¡æ•°æ®ï¼Œè¯·ç¨åé‡è¯•ã€‚")
    else:
        # é…ç½®åˆ—æ˜¾ç¤ºæ ¼å¼
        column_cfg = {
            "ä»£ç ": st.column_config.TextColumn("ä»£ç "),
            "åç§°": st.column_config.TextColumn("åç§°"),
            "æœ€æ–°ä»·": st.column_config.NumberColumn("æœ€æ–°ä»·", format="%.2f"),
            "æ¶¨è·Œå¹…": st.column_config.NumberColumn("æ¶¨è·Œå¹…", format="%.2f%%"),
            "æˆäº¤é¢": st.column_config.NumberColumn("æˆäº¤é¢", format="ï¿¥%.0f"),
            "æ¢æ‰‹ç‡": st.column_config.NumberColumn("æ¢æ‰‹ç‡", format="%.2f%%"),
            "å¸‚ç›ˆç‡-åŠ¨æ€": st.column_config.NumberColumn("PE(åŠ¨)", format="%.1f"),
        }
        
        st.dataframe(
            df_stocks,
            use_container_width=True,
            hide_index=True,
            column_config=column_cfg
        )

# ==============================================================================
# 4. ä¸»ç¨‹åºå…¥å£
# ==============================================================================
def main():
    # --- ä¾§è¾¹æ  ---
    with st.sidebar:
        st.header("âš™ï¸ å‚æ•°é…ç½®")
        top_n = st.slider("å±•ç¤ºè¡Œä¸šæ•°é‡", 10, 50, 20)
        refresh_btn = st.button("ğŸ”„ åˆ·æ–°æ•°æ®")
        
        if refresh_btn:
            st.cache_data.clear()
            st.rerun()

    st.title("ğŸš€ Aè‚¡è¡Œä¸šèµ„é‡‘æµå‘é€è§†")
    
    # 1. è·å–ä¸»æ¦œå•æ•°æ®
    df_all = DataManager.get_sector_flow_rank()
    
    if df_all.empty:
        st.error("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ç¨åé‡è¯•ã€‚")
        st.stop()

    # 2. æˆªå– Top N
    df_view = df_all.head(top_n).copy()

    # --- æ ¸å¿ƒäº¤äº’å›¾è¡¨ (Altair) ---
    st.subheader(f"ğŸ“Š çƒ­é—¨è¡Œä¸šèµ„é‡‘æµå‘ (Top {top_n})")
    st.info("ğŸ‘† ç‚¹å‡»ä¸‹æ–¹çš„æŸ±çŠ¶å›¾ï¼Œå¯æŸ¥çœ‹è¯¥è¡Œä¸šçš„æˆåˆ†è‚¡åˆ—è¡¨")

    # å®šä¹‰åŸºç¡€å›¾è¡¨
    base = alt.Chart(df_view).encode(
        x=alt.X('è¡Œä¸šåç§°', sort=None, title="è¡Œä¸šæ¿å—"),
        y=alt.Y('ä¸»åŠ›å‡€æµå…¥', title="ä¸»åŠ›å‡€æµå…¥(å…ƒ)"),
        tooltip=['è¡Œä¸šåç§°', 'ä¸»åŠ›å‡€æµå…¥', 'ä¸»åŠ›å‡€æµå…¥-å‡€å æ¯”']
    ).properties(height=450)

    # [å…³é”®ä¿®å¤] å®šä¹‰å…·åé€‰æ‹©å™¨ï¼Œç”¨äºæ•è·ç‚¹å‡»äº‹ä»¶
    # name='select_sector' æ˜¯å¿…é¡»çš„ï¼Œè¿™æ ·åœ¨ event.selection ä¸­æ‰èƒ½é€šè¿‡è¿™ä¸ªåå­—å–å€¼
    click_selection = alt.selection_point(name='select_sector', fields=['è¡Œä¸šåç§°'], on='click')

    # ç»˜åˆ¶æŸ±çŠ¶å›¾ï¼Œå¹¶ç»‘å®šé€‰æ‹©å™¨
    bars = base.mark_bar().encode(
        # é€‰ä¸­æ—¶å®Œå…¨ä¸é€æ˜ï¼Œæœªé€‰ä¸­æ—¶åŠé€æ˜
        opacity=alt.condition(click_selection, alt.value(1.0), alt.value(0.3)),
        color=alt.condition(
            alt.datum['ä¸»åŠ›å‡€æµå…¥'] > 0,
            alt.value("#f5222d"),  # çº¢
            alt.value("#52c41a")   # ç»¿
        )
    ).add_params(click_selection)

    # æ¸²æŸ“å›¾è¡¨ï¼Œon_select="rerun" è§¦å‘ç”Ÿæ•ˆ
    try:
        event = st.altair_chart(bars, use_container_width=True, on_select="rerun")
    except TypeError:
        st.altair_chart(bars, use_container_width=True)
        st.error("æ‚¨çš„ Streamlit ç‰ˆæœ¬ä¸æ”¯æŒ on_selectï¼Œè¯·å‡çº§åˆ° 1.35.0 ä»¥ä¸Šã€‚")
        return

    # --- å¤„ç†ç‚¹å‡»äº‹ä»¶ ---
    # [å…³é”®ä¿®å¤] ä¹‹å‰çš„ AttributeError æ˜¯å› ä¸ºä½¿ç”¨äº† event.selection.rows
    # æ­£ç¡®çš„åšæ³•æ˜¯æ ¹æ®é€‰æ‹©å™¨åç§° ('select_sector') ä»å­—å…¸ä¸­å–å‡ºæ•°æ®
    if event.selection and 'select_sector' in event.selection:
        selection_list = event.selection['select_sector']
        
        if selection_list and len(selection_list) > 0:
            # è·å–è¢«ç‚¹å‡»çš„è¡Œä¸šåç§°
            sector_data = selection_list[0]
            sector_name = sector_data.get("è¡Œä¸šåç§°")
            
            if sector_name:
                log_info(f"ğŸ–±ï¸ ç”¨æˆ·ç‚¹å‡»äº†: {sector_name}")
                # å¼¹å‡ºæ¨¡æ€çª—å£
                show_stock_list_dialog(sector_name)

    # --- åº•éƒ¨æ•°æ®é¢„è§ˆ ---
    with st.expander("æŸ¥çœ‹æ¦œå•æºæ•°æ®"):
        st.dataframe(df_view)

if __name__ == "__main__":
    main()
```

## å¤§ç›˜å…¨æ™¯çœ‹æ¿

- idï¼š`15`
- is_pinnedï¼š`1`
- updated_atï¼š`2026-01-15 08:00:51`
- last_run_statusï¼š`pending`
- descriptionï¼šA-Share Market Sentiment Analysis (Streamlit)

### script_content

```python

import streamlit as st
import os
import sys
import akshare as ak
import pandas as pd
import json
import datetime
import time
import traceback

try:
    sys.stderr = open(os.devnull, "w")
except Exception:
    pass

# --- Configuration ---
st.set_page_config(page_title="å¤§ç›˜å…¨æ™¯çœ‹æ¿", layout="wide")

# --- Styles ---
st.markdown("""
<style>
    .metric-card {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
        text-align: center;
    }
    .metric-value {
        font-size: 24px;
        font-weight: bold;
    }
    .metric-delta {
        font-size: 14px;
    }
    .stButton>button {
        width: 100%;
    }
</style>
""", unsafe_allow_html=True)

# --- Data Fetching (Cached) ---
@st.cache_data(ttl=60)
def load_market_data(run_token: str):
    data = {}
    logs = []

    def _try_call(label: str, fn, retries: int = 2, sleep_s: float = 0.6):
        last_err = None
        for i in range(retries + 1):
            t0 = time.time()
            try:
                v = fn()
                dt = time.time() - t0
                logs.append(f"[OK] {label} {dt:.2f}s")
                return v
            except Exception as e:
                dt = time.time() - t0
                last_err = e
                logs.append(f"[ERR] {label} {dt:.2f}s {repr(e)}")
                logs.append(traceback.format_exc())
                if i < retries:
                    time.sleep(sleep_s)
        raise last_err

    # 1. Indices (Sina is fast and stable)
    try:
        df_index = _try_call("stock_zh_index_spot_sina", lambda: ak.stock_zh_index_spot_sina())
        # Filter Key Indices
        targets = ["ä¸Šè¯æŒ‡æ•°", "æ·±è¯æˆæŒ‡", "åˆ›ä¸šæ¿æŒ‡", "ç§‘åˆ›50"] 
        # Note: Sina names might vary slightly, e.g. "ä¸Šè¯æŒ‡æ•°"
        if isinstance(df_index, pd.DataFrame) and (not df_index.empty) and ("åç§°" in df_index.columns):
            data['indices'] = df_index[df_index['åç§°'].isin(targets)].copy()
        else:
            data['indices'] = pd.DataFrame()
    except Exception as e:
        st.error(f"æŒ‡æ•°æ•°æ®è·å–å¤±è´¥: {e}")
        data['indices'] = pd.DataFrame()

    # 2. Northbound Funds
    try:
        df_north = _try_call("stock_hsgt_fund_flow_summary_em", lambda: ak.stock_hsgt_fund_flow_summary_em())
        if isinstance(df_north, pd.DataFrame):
            data['hsgt'] = df_north.copy()
        else:
            data['hsgt'] = pd.DataFrame()
        # Usually row 0 is Northbound (æ²ªè‚¡é€š+æ·±è‚¡é€š sum is not directly given, need to sum)
        # Structure: æ²ªè‚¡é€š(North), æ¸¯è‚¡é€š(South), æ·±è‚¡é€š(North), æ¸¯è‚¡é€š(South)
        # We need rows where "èµ„é‡‘æ–¹å‘" == "åŒ—å‘"
        if not df_north.empty and 'èµ„é‡‘æ–¹å‘' in df_north.columns:
            data['north'] = df_north[df_north['èµ„é‡‘æ–¹å‘'] == 'åŒ—å‘'].copy()
            data['south'] = df_north[df_north['èµ„é‡‘æ–¹å‘'] == 'å—å‘'].copy()
        else:
            data['north'] = pd.DataFrame()
            data['south'] = pd.DataFrame()
    except Exception as e:
        # Fallback
        data['hsgt'] = pd.DataFrame()
        data['north'] = pd.DataFrame()
        data['south'] = pd.DataFrame()

    # 3. Market Summary (Breadth)
    try:
        sse = _try_call("stock_sse_summary", lambda: ak.stock_sse_summary())
        szse = _try_call("stock_szse_summary", lambda: ak.stock_szse_summary())
        data['sse'] = sse
        data['szse'] = szse
    except:
        pass

    # 4. Sectors
    try:
        sectors = _try_call("stock_board_industry_name_em", lambda: ak.stock_board_industry_name_em())
        data['sectors'] = sectors
    except:
        data['sectors'] = pd.DataFrame()

    data["_logs"] = "\n".join(logs[-120:])
    return data

# --- UI Layout ---

col_header_1, col_header_2 = st.columns([3, 1])
with col_header_1:
    st.title("ğŸ“Š Aè‚¡å¤§ç›˜å…¨æ™¯ç›‘æµ‹")
    st.caption(f"æœ€åæ›´æ–°: {datetime.datetime.now().strftime('%H:%M:%S')}")

with col_header_2:
    if st.button("ğŸ”„ ç«‹å³åˆ·æ–°æ•°æ®"):
        st.session_state["_market_run_token"] = str(time.time())
        st.rerun()

if "_market_run_token" not in st.session_state:
    st.session_state["_market_run_token"] = str(time.time())

if "_market_first_enter_done" not in st.session_state:
    st.session_state["_market_first_enter_done"] = True
    st.session_state["_market_run_token"] = str(time.time())

# Load Data
with st.spinner("æ­£åœ¨è¿æ¥è¡Œæƒ…ä¸­å¿ƒ..."):
    market_data = load_market_data(st.session_state["_market_run_token"])

with st.expander("è¿è¡Œæ—¥å¿—", expanded=False):
    st.code(market_data.get("_logs", ""), language="text")

with st.expander("å¯¼å‡ºæ•°æ®(JSON)", expanded=False):
    indices_df_export = market_data.get("indices", pd.DataFrame())
    hsgt_df_export = market_data.get("hsgt", pd.DataFrame())
    sectors_df_export = market_data.get("sectors", pd.DataFrame())

    include_full_sectors = st.checkbox("åŒ…å«å…¨é‡è¡Œä¸šåˆ—è¡¨", value=False)
    export_payload = {
        "generated_at": datetime.datetime.now().isoformat(timespec="seconds"),
        "indices": indices_df_export.to_dict(orient="records") if isinstance(indices_df_export, pd.DataFrame) else [],
        "fund_flow": hsgt_df_export.to_dict(orient="records") if isinstance(hsgt_df_export, pd.DataFrame) else [],
        "sectors_top10": [],
        "sectors_bottom10": [],
        "sectors": [],
    }

    if isinstance(sectors_df_export, pd.DataFrame) and (not sectors_df_export.empty) and ("æ¶¨è·Œå¹…" in sectors_df_export.columns):
        top_10_export = sectors_df_export.sort_values(by="æ¶¨è·Œå¹…", ascending=False).head(10)
        bottom_10_export = sectors_df_export.sort_values(by="æ¶¨è·Œå¹…", ascending=True).head(10)
        export_payload["sectors_top10"] = top_10_export.to_dict(orient="records")
        export_payload["sectors_bottom10"] = bottom_10_export.to_dict(orient="records")
        if include_full_sectors:
            export_payload["sectors"] = sectors_df_export.to_dict(orient="records")

    export_json = json.dumps(export_payload, ensure_ascii=False, indent=2, default=str)
    st.download_button(
        "â¬‡ï¸ å¯¼å‡º JSON",
        data=export_json,
        file_name=f"market_dashboard_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json",
        use_container_width=True,
    )
    st.code(export_json, language="json")

# --- Section 1: Key Indices ---
st.subheader("æ ¸å¿ƒæŒ‡æ•°")
cols = st.columns(4)
indices_df = market_data.get('indices', pd.DataFrame())

if not indices_df.empty:
    # Sort to ensure order if possible, or just iterate
    # Target order: SH, SZ, CYB, KC50
    target_order = ["ä¸Šè¯æŒ‡æ•°", "æ·±è¯æˆæŒ‡", "åˆ›ä¸šæ¿æŒ‡", "ç§‘åˆ›50"]
    
    for i, name in enumerate(target_order):
        row = indices_df[indices_df['åç§°'] == name]
        if not row.empty:
            price = pd.to_numeric(row.iloc[0]['æœ€æ–°ä»·'], errors='coerce')
            change = pd.to_numeric(row.iloc[0]['æ¶¨è·Œå¹…'], errors='coerce')
            with cols[i]:
                st.metric(label=name, value=f"{price:.2f}" if pd.notna(price) else "-", delta=f"{change:.2f}%" if pd.notna(change) else None)
else:
    st.warning("æš‚æ— æŒ‡æ•°æ•°æ®")

st.divider()

# --- Section 2: Market Sentiment & Funds ---
col_fund, col_breadth = st.columns([1, 2])

with col_fund:
    st.subheader("ğŸ’¸ èµ„é‡‘é£å‘")
    hsgt_df = market_data.get('hsgt', pd.DataFrame())
    if not hsgt_df.empty:
        direction = st.segmented_control(
            "èµ„é‡‘æ–¹å‘",
            options=["åŒ—å‘", "å—å‘", "å…¨éƒ¨"],
            default="åŒ—å‘",
            label_visibility="collapsed",
        )
        if direction == "åŒ—å‘":
            df_flow = hsgt_df[hsgt_df.get('èµ„é‡‘æ–¹å‘', '') == 'åŒ—å‘'].copy()
        elif direction == "å—å‘":
            df_flow = hsgt_df[hsgt_df.get('èµ„é‡‘æ–¹å‘', '') == 'å—å‘'].copy()
        else:
            df_flow = hsgt_df.copy()

        total_in = float("nan")
        total_buy = float("nan")
        try:
            if 'èµ„é‡‘å‡€æµå…¥' in df_flow.columns:
                total_in = pd.to_numeric(df_flow['èµ„é‡‘å‡€æµå…¥'], errors='coerce').sum(min_count=1)
            if 'æˆäº¤å‡€ä¹°é¢' in df_flow.columns:
                total_buy = pd.to_numeric(df_flow['æˆäº¤å‡€ä¹°é¢'], errors='coerce').sum(min_count=1)
        except:
            pass

        status_hint = ""
        try:
            if 'äº¤æ˜“çŠ¶æ€' in df_flow.columns:
                status_vals = [str(x) for x in pd.unique(df_flow['äº¤æ˜“çŠ¶æ€'].dropna()).tolist()]
                if status_vals:
                    status_hint = f"äº¤æ˜“çŠ¶æ€: {', '.join(status_vals)}"
        except:
            pass

        if pd.isna(total_buy) and pd.isna(total_in):
            st.info("èµ„é‡‘æ¥å£è¿”å›ä¸ºç©ºæˆ–å­—æ®µæ— æ³•è§£æ")
        elif pd.notna(total_buy):
            st.metric(
                f"{direction}æˆäº¤å‡€ä¹°é¢(åˆè®¡)",
                f"{total_buy:.2f}",
                delta="æµå…¥" if total_buy > 0 else "æµå‡º"
            )
        elif pd.notna(total_in):
            st.metric(
                f"{direction}èµ„é‡‘å‡€æµå…¥(åˆè®¡)",
                f"{total_in:.2f}",
                delta="æµå…¥" if total_in > 0 else "æµå‡º"
            )

        if direction == "åŒ—å‘" and (pd.notna(total_buy) and abs(float(total_buy)) < 1e-9) and status_hint:
            st.caption(f"æç¤ºï¼šå½“å‰åŒ—å‘æ•°æ®ä¸º 0ï¼Œ{status_hint}ï¼ˆå¯èƒ½ä¼‘å¸‚/ä¸Šæ¸¸æš‚æ— æ•°æ®ï¼‰")
        elif status_hint:
            st.caption(status_hint)

        show_cols = [c for c in ['äº¤æ˜“æ—¥', 'æ¿å—', 'èµ„é‡‘æ–¹å‘', 'äº¤æ˜“çŠ¶æ€', 'èµ„é‡‘å‡€æµå…¥', 'æˆäº¤å‡€ä¹°é¢', 'å½“æ—¥èµ„é‡‘ä½™é¢'] if c in df_flow.columns]
        if show_cols:
            st.dataframe(df_flow[show_cols], hide_index=True)
        else:
            st.dataframe(df_flow, hide_index=True)
    else:
        st.info("èµ„é‡‘æ•°æ®ä¸å¯ç”¨")

with col_breadth:
    st.subheader("ğŸŒ¡ï¸ å¸‚åœºæ¸©åº¦")
    # Calculate approximate Up/Down from Summary if available, or just verify
    # SSE Summary has 'ä¸Šå¸‚è‚¡ç¥¨' but not Up/Down count directly. 
    # SZSE Summary also general.
    # To get exact Up/Down, we need a snapshot or estimate.
    # Let's use Sectors as a proxy for heat.
    
    sectors = market_data.get('sectors', pd.DataFrame())
    if not sectors.empty:
        up_sectors = len(sectors[sectors['æ¶¨è·Œå¹…'] > 0])
        down_sectors = len(sectors[sectors['æ¶¨è·Œå¹…'] < 0])
        total_sectors = len(sectors)
        
        st.write(f"è¡Œä¸šæ¿å—æ¶¨è·Œåˆ†å¸ƒ: ğŸŸ¥ {up_sectors} æ¶¨ / ğŸŸ© {down_sectors} è·Œ")
        
        # Simple progress bar for sentiment
        sentiment_score = up_sectors / total_sectors if total_sectors > 0 else 0.5
        st.progress(sentiment_score, text=f"å¸‚åœºæƒ…ç»ª (è¡Œä¸šç»´åº¦): {int(sentiment_score*100)}%")
    else:
        st.info("æ¿å—æ•°æ®ä¸å¯ç”¨")

st.divider()

# --- Section 3: Sector Performance ---
st.subheader("ğŸš€ è¡Œä¸šçƒ­åº¦æ¦œ")

sectors = market_data.get('sectors', pd.DataFrame())
if not sectors.empty:
    # Top 10 Gainers
    top_10 = sectors.sort_values(by="æ¶¨è·Œå¹…", ascending=False).head(10)
    # Bottom 10 Losers
    bottom_10 = sectors.sort_values(by="æ¶¨è·Œå¹…", ascending=True).head(10)
    
    col_top, col_bottom = st.columns(2)
    
    with col_top:
        st.markdown("**æ¶¨å¹… Top 10**")
        df_up = top_10[['æ¿å—åç§°', 'æ¶¨è·Œå¹…']].set_index('æ¿å—åç§°')
        st.bar_chart(df_up, height=380)
        
    with col_bottom:
        st.markdown("**è·Œå¹… Top 10**")
        df_down = bottom_10[['æ¿å—åç§°', 'æ¶¨è·Œå¹…']].set_index('æ¿å—åç§°')
        st.bar_chart(df_down, height=380)
else:
    st.error("æ— æ³•åŠ è½½è¡Œä¸šæ•°æ®")
```

## æ ¹æ®è¡Œä¸šå­—æ®µè·å–è‚¡ç¥¨é›†åˆ

- idï¼š`4`
- is_pinnedï¼š`1`
- updated_atï¼š`2026-01-15 08:00:32`
- last_run_statusï¼š`pending`

### script_content

```python
# Write your Python script here
# Define "df" (DataFrame) or "result" (List) for table
# Define "chart" (Dict) for visualization
''' ['èƒ½æºé‡‘å±', 'ç»ç’ƒç»çº¤', 'ç”µæ± ', 'å°é‡‘å±', 'åŒ–è‚¥è¡Œä¸š', 'é£ç”µè®¾å¤‡', 'é“è·¯å…¬è·¯', 'æ—…æ¸¸é…’åº—', 
'ç”µå­åŒ–å­¦å“', 'æ±½è½¦é›¶éƒ¨ä»¶', 'æˆ¿åœ°äº§æœåŠ¡', 'å…‰ä¼è®¾å¤‡', 'æœ‰è‰²é‡‘å±', 'åŒ–å­¦åˆ¶å“', 'åŒ–å­¦åŸæ–™', 
'éé‡‘å±ææ–™', 'æ±½è½¦æ•´è½¦', 'é£Ÿå“é¥®æ–™', 'å…¬ç”¨äº‹ä¸š', 'ç‰©æµè¡Œä¸š', 'å¡‘æ–™åˆ¶å“', 'é’¢é“è¡Œä¸š', 
'æ±½è½¦æœåŠ¡', 'å†œè¯å…½è¯', 'æ¶ˆè´¹ç”µå­', 'è´µé‡‘å±', 'æ©¡èƒ¶åˆ¶å“', 'å·¥ç¨‹æœºæ¢°', 'äº¤è¿è®¾å¤‡', 'èˆªç©ºæœºåœº',
 'ç”µæœº', 'èˆªè¿æ¸¯å£', 'å…‰å­¦å…‰ç”µå­', 'å•†ä¸šç™¾è´§', 'ç¯ä¿è¡Œä¸š', 'å†œç‰§é¥²æ¸”', 'ç»¼åˆè¡Œä¸š', 'æˆ¿åœ°äº§å¼€å‘', 
 'å·¥ç¨‹å’¨è¯¢æœåŠ¡', 'ç¾å®¹æŠ¤ç†', 'ç”µåŠ›è¡Œä¸š', 'ç…¤ç‚­è¡Œä¸š', 'ä¸“ç”¨è®¾å¤‡', 'å®¶ç”µè¡Œä¸š', 'è£…ä¿®è£…é¥°', 'é€ çº¸å°åˆ·', 
 'ä¸“ä¸šæœåŠ¡', 'ç”µå­å…ƒä»¶', 'é€šç”¨è®¾å¤‡', 'é…¿é…’è¡Œä¸š', 'åŠå¯¼ä½“', 'å·¥ç¨‹å»ºè®¾', 'é“¶è¡Œ', 'ä¸­è¯', 'å®¶ç”¨è½»å·¥', 
 'ä¿é™©', 'è´¸æ˜“è¡Œä¸š', 'åŒ–çº¤è¡Œä¸š', 'çŸ³æ²¹è¡Œä¸š', 'ç”µç½‘è®¾å¤‡', 'æ¸¸æˆ', 'ç‡ƒæ°”', 'æ°´æ³¥å»ºæ',
  'åŒ»ç–—å™¨æ¢°', 'çººç»‡æœè£…', 'è£…ä¿®å»ºæ', 'åŒ–å­¦åˆ¶è¯', 'åŒ»ç–—æœåŠ¡', 'ä»ªå™¨ä»ªè¡¨', 'åŒ…è£…ææ–™', 
  'ç”µæºè®¾å¤‡', 'ç”Ÿç‰©åˆ¶å“', 'èˆªå¤©èˆªç©º', 'è¯åˆ¸', 'èˆ¹èˆ¶åˆ¶é€ ', 'å¤šå…ƒé‡‘è', 'è®¡ç®—æœºè®¾å¤‡', 
  'é‡‡æ˜è¡Œä¸š', 'ç å®é¦–é¥°', 'è½¯ä»¶å¼€å‘', 'é€šä¿¡æœåŠ¡', 'æ•™è‚²', 'äº’è”ç½‘æœåŠ¡', 'åŒ»è¯å•†ä¸š', 'æ–‡åŒ–ä¼ åª’', 'é€šä¿¡è®¾å¤‡']'''

import akshare as ak
import pandas as pd

# names_df = ak.stock_board_industry_name_em()
# names=names_df["æ¿å—åç§°"].drop_duplicates()
# print(names.tolist())
# print(names_df)

df=ak.stock_board_industry_cons_em(symbol="é€ çº¸å°åˆ·")


print("Response:", df.to_json())
```

## å±±è°·ç‹™å‡»è¯„åˆ†ï¼ˆæŒ‰è‚¡ç¥¨ä»£ç ï¼‰

- idï¼š`14`
- is_pinnedï¼š`1`
- updated_atï¼š`2026-01-15 08:00:23`
- last_run_statusï¼š`pending`
- descriptionï¼šå®Œå…¨å¤ç”¨â€˜å±±è°·ç‹™å‡»é€‰è‚¡ç­–ç•¥_optimizedâ€™çš„ç­›é€‰/è¯„åˆ†é€»è¾‘ï¼Œå¯¹æŒ‡å®šè‚¡ç¥¨è¾“å‡º0-100è¯„åˆ†ä¸æ˜ç»†

### script_content

```python
import pandas as pd
import numpy as np
import datetime

codes = ["600746"]

RECENT_VOLUME_DAYS = 5
VOLUME_BASE_DAYS = 120

CAP_SMALL = 100 * 1e8
CAP_LARGE = 500 * 1e8

VOL_RANK_LARGE = 0.25
VOL_RANK_MID = 0.15
VOL_RANK_SMALL = 0.10

MIN_TURNOVER_AMOUNT = 30000000
MAX_PRICE_CHANGE = 6.0

THRESHOLD_HIGH_QUALITY = 7
THRESHOLD_POTENTIAL = 4

AR_SPREAD_WINDOW = 20
AR_SPREAD_LOOKBACK = 120
RSV_WINDOW = 20
RSV_LOOKBACK = 120
ILLIQ_COMPOSITE_THRESHOLD = 0.70
AR_SPREAD_RANK_SKIP = 0.90

BB_WINDOW = 5

OVERHEAD_VOL_WINDOW = 20
DRAWDOWN_THRESHOLD = 0.20

SCORE_CRITERIA = {
    "volume_extreme": 3,
    "volume_high": 1,
    "macd_div": 3,
    "rsi_div": 2,
    "illiq_composite": 2,
    "ofi_confirm": 1,
    "vrp_signal": 2,
    "rebound_confirm": 2,
    "sector_fund_flow_strong": 2,
    "sector_fund_flow_ok": 1,
    "weibo_panic": 1,
    "heat_penalty": 1,
    "weibo_hype_penalty": 1,
}

MAX_POS_SCORE = (
    SCORE_CRITERIA["volume_extreme"]
    + SCORE_CRITERIA["vrp_signal"]
    + SCORE_CRITERIA["macd_div"]
    + SCORE_CRITERIA["rsi_div"]
    + SCORE_CRITERIA["illiq_composite"]
    + SCORE_CRITERIA["ofi_confirm"]
    + SCORE_CRITERIA["rebound_confirm"]
    + SCORE_CRITERIA["sector_fund_flow_strong"]
    + SCORE_CRITERIA["weibo_panic"]
)


def _normalize_symbol(code: str) -> str:
    s = "" if code is None else str(code).strip()
    if len(s) >= 8 and s[:2].lower() in ("sh", "sz", "bj"):
        return s[2:]
    return s.zfill(6) if s.isdigit() and len(s) <= 6 else s


def _to_num(s: pd.Series) -> pd.Series:
    return pd.to_numeric(s, errors="coerce")


def _kalman_filter_1d(values: pd.Series, q: float = 1e-5, r_scale: float = 0.20):
    v = pd.to_numeric(values, errors="coerce").to_numpy(dtype=float)
    if v.size == 0:
        return values
    if not np.isfinite(v).any():
        return values

    first_finite_idx = int(np.argmax(np.isfinite(v)))

    dv = np.diff(v)
    dv = dv[np.isfinite(dv)]
    base_var = float(np.nanvar(dv)) if dv.size else 0.0
    r = max(1e-9, r_scale * base_var)

    x = float(v[first_finite_idx])
    p = 1.0
    out = np.empty_like(v, dtype=float)

    for i in range(v.size):
        p = p + q
        if np.isfinite(v[i]):
            k = p / (p + r)
            x = x + k * (v[i] - x)
            p = (1.0 - k) * p
        out[i] = x

    return pd.Series(out, index=values.index)


def _local_mins(values: np.ndarray, order: int) -> list:
    if values.size == 0:
        return []
    order = int(order)
    if order <= 0:
        return []
    mins = []
    n = int(values.size)
    for i in range(order, n - order):
        v = values[i]
        if not np.isfinite(v):
            continue
        left = values[i - order : i]
        right = values[i + 1 : i + 1 + order]
        if np.isfinite(left).all() and np.isfinite(right).all() and (v < left.min()) and (v < right.min()):
            mins.append(i)
    return mins


def _get_bb_troughs(series: pd.Series, window: int = BB_WINDOW) -> list:
    data = pd.to_numeric(series, errors="coerce").to_numpy(dtype=float)
    local_mins = _local_mins(data, window)

    refined = []
    if local_mins:
        refined.append(local_mins[0])
        for i in range(1, len(local_mins)):
            if local_mins[i] - refined[-1] >= window:
                refined.append(local_mins[i])
            else:
                if data[local_mins[i]] < data[refined[-1]]:
                    refined[-1] = local_mins[i]
    return refined


def _macd(close: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9):
    c = pd.to_numeric(close, errors="coerce")
    ema_fast = c.ewm(span=fast, adjust=False).mean()
    ema_slow = c.ewm(span=slow, adjust=False).mean()
    macd_line = ema_fast - ema_slow
    signal_line = macd_line.ewm(span=signal, adjust=False).mean()
    hist = macd_line - signal_line
    return macd_line, signal_line, hist


def _rsi(close: pd.Series, period: int = 14) -> pd.Series:
    c = pd.to_numeric(close, errors="coerce")
    delta = c.diff()
    gain = delta.clip(lower=0)
    loss = (-delta).clip(lower=0)
    avg_gain = gain.ewm(alpha=1 / period, adjust=False, min_periods=period).mean()
    avg_loss = loss.ewm(alpha=1 / period, adjust=False, min_periods=period).mean()
    rs = avg_gain / (avg_loss + 1e-12)
    return 100 - (100 / (1 + rs))


def detect_dynamic_divergence(smooth_p: pd.Series, indicator: pd.Series) -> bool:
    if len(smooth_p) < 60:
        return False

    troughs = _get_bb_troughs(smooth_p)
    if len(troughs) < 2:
        return False

    last_idx = troughs[-1]
    prev_idx = troughs[-2]

    if (len(smooth_p) - 1) - last_idx > 15:
        return False

    p_last = float(smooth_p.iloc[last_idx])
    p_prev = float(smooth_p.iloc[prev_idx])
    i_last = float(indicator.iloc[last_idx])
    i_prev = float(indicator.iloc[prev_idx])

    if p_last <= p_prev * 1.02 and i_last > i_prev * 1.05:
        recent_acceleration = float(smooth_p.diff().diff().iloc[last_idx])
        if np.isfinite(recent_acceleration) and recent_acceleration > 0:
            return True
    return False


def calc_composite_illiq(close: pd.Series, amount: pd.Series, high: pd.Series, low: pd.Series) -> float:
    if len(close) < 20:
        return 0.0

    c = pd.to_numeric(close, errors="coerce")
    amt = pd.to_numeric(amount, errors="coerce")
    h = pd.to_numeric(high, errors="coerce")
    l = pd.to_numeric(low, errors="coerce")

    rets = c.pct_change().abs()
    amihud = rets / (amt + 1e-9) * 1e8

    hl_ratio = (h - l) / (c + 1e-9)

    curr_amihud = float(amihud.iloc[-20:].mean())
    curr_hl = float(hl_ratio.iloc[-20:].mean())

    hist_amihud = amihud.iloc[-120:]
    hist_hl = hl_ratio.iloc[-120:]

    amihud_rank = float((hist_amihud <= curr_amihud).mean()) if len(hist_amihud) else 0.0
    hl_rank = float((hist_hl <= curr_hl).mean()) if len(hist_hl) else 0.0

    return (amihud_rank + hl_rank) / 2.0


def calc_ar_spread_rank(high: pd.Series, low: pd.Series, close: pd.Series):
    h = pd.to_numeric(high, errors="coerce")
    l = pd.to_numeric(low, errors="coerce")
    c = pd.to_numeric(close, errors="coerce")

    h = np.log(h.where(h > 0))
    l = np.log(l.where(l > 0))
    c = np.log(c.where(c > 0))

    eta = (h + l) / 2.0
    term = 4.0 * (c - eta) * (c.shift(1) - eta.shift(1))
    ar = np.sqrt(np.maximum(term, 0.0))
    ar_roll = ar.rolling(window=AR_SPREAD_WINDOW, min_periods=max(3, AR_SPREAD_WINDOW // 3)).mean()

    curr = ar_roll.iloc[-1] if len(ar_roll) else np.nan
    hist = ar_roll.iloc[-AR_SPREAD_LOOKBACK:].dropna()

    rank = float((hist <= curr).mean()) if len(hist) and pd.notna(curr) else np.nan
    return rank, float(curr) if pd.notna(curr) else np.nan


def calculate_downside_rsv_rank(close: pd.Series):
    c = pd.to_numeric(close, errors="coerce").where(lambda x: x > 0)
    r = c.pct_change()
    down = np.minimum(r, 0.0) ** 2
    tot = r**2

    down_sum = down.rolling(window=RSV_WINDOW, min_periods=max(3, RSV_WINDOW // 3)).sum()
    tot_sum = tot.rolling(window=RSV_WINDOW, min_periods=max(3, RSV_WINDOW // 3)).sum()

    ratio = down_sum / (tot_sum + 1e-9)
    ratio = ratio.replace([np.inf, -np.inf], np.nan)

    last = ratio.iloc[-1] if len(ratio) else np.nan
    hist = ratio.iloc[-RSV_LOOKBACK:].dropna()

    rank = float((hist <= last).mean()) if len(hist) and pd.notna(last) else np.nan
    return rank, float(last) if pd.notna(last) else np.nan


def check_overhead_supply(close: pd.Series, volume: pd.Series, amount: pd.Series, current_price: float) -> bool:
    if len(close) < 252:
        return False

    vol_20 = pd.to_numeric(volume, errors="coerce").rolling(20).sum()
    amt_20 = pd.to_numeric(amount, errors="coerce").rolling(20).sum()

    typical_px = float(pd.to_numeric(close, errors="coerce").tail(60).median())
    typical_amt_per_vol = float(
        (pd.to_numeric(amount, errors="coerce") / (pd.to_numeric(volume, errors="coerce") + 1e-9)).tail(60).median()
    )
    vol_unit = 100.0 if (
        np.isfinite(typical_px)
        and typical_px > 0
        and np.isfinite(typical_amt_per_vol)
        and typical_amt_per_vol > typical_px * 20.0
    ) else 1.0

    vwap_20 = amt_20 / (vol_20 * vol_unit + 1e-9)
    current_vwap = float(vwap_20.iloc[-1])
    prev_vwap = float(vwap_20.iloc[-2])

    vwap_slope = current_vwap - prev_vwap
    is_above_cost = (current_price > current_vwap) or (vwap_slope > 0)

    high_52w = float(pd.to_numeric(close, errors="coerce").rolling(252).max().iloc[-1])
    if not np.isfinite(high_52w) or high_52w <= 0:
        return False

    drawdown = (high_52w - float(current_price)) / high_52w
    is_deep_enough = drawdown > DRAWDOWN_THRESHOLD

    return bool(is_above_cost and is_deep_enough)


def dynamic_volume_score(volume: pd.Series, mkt_cap: float):
    if len(volume) < 120:
        return 0, 0.0

    if mkt_cap > CAP_LARGE:
        threshold = VOL_RANK_LARGE
    elif mkt_cap < CAP_SMALL:
        threshold = VOL_RANK_SMALL
    else:
        threshold = VOL_RANK_MID

    v = pd.to_numeric(volume, errors="coerce")
    curr_vol = float(v.iloc[-RECENT_VOLUME_DAYS:].median())
    hist_vol = v.iloc[-VOLUME_BASE_DAYS:]
    vol_rank = float((hist_vol <= curr_vol).mean())

    if vol_rank < threshold:
        return SCORE_CRITERIA["volume_extreme"], vol_rank
    if vol_rank < 0.40:
        return SCORE_CRITERIA["volume_high"], vol_rank
    return 0, vol_rank


def calculate_vrp_score(close: pd.Series):
    if len(close) < 20:
        return 0, 0.0

    c = pd.to_numeric(close, errors="coerce")
    rets = c.pct_change()
    rv = rets.rolling(5).std()
    iv_proxy = rets.rolling(20).std()

    vrp = iv_proxy - rv

    curr_vrp = vrp.iloc[-1]
    hist_vrp = vrp.iloc[-120:]

    vrp_rank = float((hist_vrp <= curr_vrp).mean()) if len(hist_vrp) and pd.notna(curr_vrp) else 0.0
    if vrp_rank > 0.8:
        return SCORE_CRITERIA["vrp_signal"], vrp_rank
    return 0, vrp_rank


def calculate_ofi_signal(open_s: pd.Series, close_s: pd.Series, vol_s: pd.Series):
    o = pd.to_numeric(open_s, errors="coerce")
    c = pd.to_numeric(close_s, errors="coerce")
    v = pd.to_numeric(vol_s, errors="coerce").fillna(0.0)

    diff = c - o
    sgn = np.sign(diff)
    ofi = sgn * v

    ofi_sum = ofi.rolling(10).sum()
    vol_sum = v.rolling(10).sum()
    ratio = ofi_sum / (vol_sum.abs() + 1e-9)

    last_ratio = float(ratio.iloc[-1]) if len(ratio) else 0.0
    if np.isfinite(last_ratio) and last_ratio > 0.1:
        return SCORE_CRITERIA["ofi_confirm"], last_ratio
    return 0, last_ratio


def _clamp(v: float, lo: float, hi: float) -> float:
    if v is None:
        return lo
    try:
        x = float(v)
    except Exception:
        return lo
    if not np.isfinite(x):
        return lo
    return max(lo, min(hi, x))


def _get_name(symbol6: str) -> str:
    try:
        info = ak.stock_individual_info_em(symbol=symbol6)
        if info is not None and (not info.empty) and ("item" in info.columns) and ("value" in info.columns):
            m = dict(zip(info["item"].astype(str), info["value"].astype(str)))
            v = m.get("è‚¡ç¥¨ç®€ç§°") or m.get("è¯åˆ¸ç®€ç§°") or m.get("è‚¡ç¥¨åç§°")
            return str(v) if v else ""
    except Exception:
        pass
    return ""


industry_cache = {}

def _get_industry(symbol6: str):
    sym = _normalize_symbol(symbol6)
    if sym in industry_cache:
        return industry_cache[sym]
    industry = None
    try:
        info_df = ak.stock_individual_info_em(symbol=sym)
        if info_df is not None and not info_df.empty and "item" in info_df.columns and "value" in info_df.columns:
            mask = info_df["item"].astype(str) == "è¡Œä¸š"
            if mask.any():
                industry = str(info_df.loc[mask, "value"].iloc[0]).strip() or None
    except Exception:
        industry = None
    industry_cache[sym] = industry
    return industry


print("é€»è¾‘æ¥æº: å±±è°·ç‹™å‡»é€‰è‚¡ç­–ç•¥_optimized")
print("codes=", codes)

spot_map = {}
try:
    df_spot = ak.stock_zh_a_spot_em()
    if df_spot is not None and not df_spot.empty and "ä»£ç " in df_spot.columns:
        df_spot = df_spot.copy()
        if "åç§°" in df_spot.columns:
            df_spot = df_spot[~df_spot["åç§°"].astype(str).str.contains("ST|é€€", na=False)]
        df_spot["ä»£ç "] = df_spot["ä»£ç "].astype(str).map(_normalize_symbol)
        spot_map = {str(r["ä»£ç "]): r for _, r in df_spot.iterrows()}
except Exception:
    spot_map = {}

sector_fund_flow_map = {}
try:
    fund_flow_df = ak.stock_sector_fund_flow_rank(indicator="5æ—¥", sector_type="è¡Œä¸šèµ„é‡‘æµ")
    if fund_flow_df is not None and not fund_flow_df.empty and "åç§°" in fund_flow_df.columns:
        col = "5æ—¥ä¸»åŠ›å‡€æµå…¥-å‡€å æ¯”"
        if col in fund_flow_df.columns:
            ff = fund_flow_df[["åç§°", col]].copy()
            ff["åç§°"] = ff["åç§°"].astype(str)
            ff[col] = pd.to_numeric(ff[col], errors="coerce")
            ff = ff.dropna(subset=["åç§°", col])
            sector_fund_flow_map = {str(r["åç§°"]): float(r[col]) for _, r in ff.iterrows()}
except Exception:
    sector_fund_flow_map = {}

hot_rank_map = {}
try:
    hot_df = ak.stock_hot_rank_em()
    if hot_df is not None and not hot_df.empty and "ä»£ç " in hot_df.columns and "å½“å‰æ’å" in hot_df.columns:
        hd = hot_df[["ä»£ç ", "å½“å‰æ’å"]].copy()
        hd["ä»£ç "] = hd["ä»£ç "].astype(str).map(_normalize_symbol)
        hd["å½“å‰æ’å"] = pd.to_numeric(hd["å½“å‰æ’å"], errors="coerce")
        hd = hd.dropna(subset=["ä»£ç ", "å½“å‰æ’å"])
        hot_rank_map = {str(r["ä»£ç "]): int(r["å½“å‰æ’å"]) for _, r in hd.iterrows()}
except Exception:
    hot_rank_map = {}

weibo_rate_map = {}
try:
    weibo_df = ak.stock_js_weibo_report(time_period="CNHOUR24")
    if weibo_df is not None and not weibo_df.empty and "name" in weibo_df.columns and "rate" in weibo_df.columns:
        wb = weibo_df[["name", "rate"]].copy()
        wb["name"] = wb["name"].astype(str)
        wb["rate"] = pd.to_numeric(wb["rate"], errors="coerce")
        wb = wb.dropna(subset=["name", "rate"])
        weibo_rate_map = {str(r["name"]): float(r["rate"]) for _, r in wb.iterrows()}
except Exception:
    weibo_rate_map = {}


def _calc_one(code: str) -> dict:
    symbol = _normalize_symbol(code)
    spot = spot_map.get(symbol)

    name = ""
    current_price = None
    pct_chg = None
    amount_rt = None
    mkt_cap = None

    if spot is not None:
        try:
            name = str(spot.get("åç§°", "") or "")
            current_price = float(spot.get("æœ€æ–°ä»·")) if spot.get("æœ€æ–°ä»·") is not None else None
            pct_chg = float(spot.get("æ¶¨è·Œå¹…")) if spot.get("æ¶¨è·Œå¹…") is not None else None
            amount_rt = float(spot.get("æˆäº¤é¢")) if spot.get("æˆäº¤é¢") is not None else None
            mkt_cap = float(spot.get("æµé€šå¸‚å€¼")) if spot.get("æµé€šå¸‚å€¼") is not None else None
        except Exception:
            pass

    if not name:
        name = _get_name(symbol)

    prefilter_reason = ""
    if pct_chg is not None and np.isfinite(pct_chg) and abs(pct_chg) > MAX_PRICE_CHANGE:
        prefilter_reason = f"æ¶¨è·Œå¹…è¶…é˜ˆå€¼({pct_chg:.2f}%)"
    if amount_rt is not None and np.isfinite(amount_rt) and amount_rt < MIN_TURNOVER_AMOUNT:
        prefilter_reason = prefilter_reason or f"æˆäº¤é¢è¿‡ä½({amount_rt:.0f})"

    end_date = datetime.datetime.now()
    start_date = end_date - datetime.timedelta(days=730)
    start_date_str = start_date.strftime("%Y%m%d")
    end_date_str = end_date.strftime("%Y%m%d")

    df_hist = ak.stock_zh_a_hist(symbol=symbol, period="daily", start_date=start_date_str, end_date=end_date_str, adjust="qfq")
    if df_hist is None or df_hist.empty or len(df_hist) < 120:
        return {
            "ä»£ç ": symbol,
            "åç§°": name,
            "ç°ä»·": current_price,
            "æ¶¨è·Œ%": pct_chg,
            "è¯„åˆ†": None,
            "è¯„åˆ†(0-100)": None,
            "å…¥é€‰": False,
            "åŸå› ": prefilter_reason or "å†å²æ•°æ®ä¸è¶³",
        }

    df_hist = df_hist.copy()
    for col in ["å¼€ç›˜", "æ”¶ç›˜", "æœ€é«˜", "æœ€ä½", "æˆäº¤é‡", "æˆäº¤é¢"]:
        if col in df_hist.columns:
            df_hist[col] = pd.to_numeric(df_hist[col], errors="coerce")

    close = df_hist["æ”¶ç›˜"]
    open_ = df_hist["å¼€ç›˜"]
    high = df_hist["æœ€é«˜"]
    low = df_hist["æœ€ä½"]
    volume = df_hist["æˆäº¤é‡"]
    amount = df_hist["æˆäº¤é¢"]

    if current_price is None or (not np.isfinite(current_price)):
        try:
            current_price = float(close.iloc[-1])
        except Exception:
            current_price = None

    safe = check_overhead_supply(close, volume, amount, float(current_price) if current_price is not None else float("nan"))
    if not safe:
        return {
            "ä»£ç ": symbol,
            "åç§°": name,
            "ç°ä»·": current_price,
            "æ¶¨è·Œ%": pct_chg,
            "è¯„åˆ†": 0,
            "è¯„åˆ†(0-100)": 0.0,
            "å…¥é€‰": False,
            "åŸå› ": prefilter_reason or "åŠå±±è…°è§„é¿æœªé€šè¿‡",
        }

    score = 0
    signals = []

    smooth_p = _kalman_filter_1d(close)

    v_score, v_rank = dynamic_volume_score(volume, float(mkt_cap) if mkt_cap is not None else 100e8)
    if v_score > 0:
        score += v_score
        signals.append(f"ç¼©é‡({int(v_rank*100)}%)")

    vrp_score, vrp_rank = calculate_vrp_score(close)
    if vrp_score > 0:
        score += vrp_score
        signals.append("VRPææ…Œ")

    macd_line, macd_sig, _macd_hist = _macd(smooth_p)
    if detect_dynamic_divergence(smooth_p, macd_line):
        score += SCORE_CRITERIA["macd_div"]
        signals.append("MACDåº•")

    rsi14 = _rsi(smooth_p, 14)
    if detect_dynamic_divergence(smooth_p, rsi14):
        score += SCORE_CRITERIA["rsi_div"]
        signals.append("RSIåº•")

    comp_illiq = calc_composite_illiq(close, amount, high, low)
    if comp_illiq > ILLIQ_COMPOSITE_THRESHOLD:
        score += SCORE_CRITERIA["illiq_composite"]
        signals.append("ILLIQå¸æ”¶")

    ar_rank, ar_spread = calc_ar_spread_rank(high, low, close)
    if pd.notna(ar_rank) and float(ar_rank) >= AR_SPREAD_RANK_SKIP:
        return {
            "ä»£ç ": symbol,
            "åç§°": name,
            "ç°ä»·": current_price,
            "æ¶¨è·Œ%": pct_chg,
            "è¯„åˆ†": score,
            "è¯„åˆ†(0-100)": round(_clamp(score / MAX_POS_SCORE, 0.0, 1.0) * 100.0, 2),
            "å…¥é€‰": False,
            "åŸå› ": "ARåˆ†ä½è¿‡é«˜(è·³è¿‡)",
            "ç¼©é‡åˆ†ä½": round(v_rank, 2),
            "VRP": round(vrp_rank, 2),
            "ILLIQ": round(comp_illiq, 2),
            "ARåˆ†ä½": round(float(ar_rank), 2),
        }

    down_rank, down_ratio = calculate_downside_rsv_rank(close)
    if pd.notna(down_rank) and float(down_rank) > 0.80:
        return {
            "ä»£ç ": symbol,
            "åç§°": name,
            "ç°ä»·": current_price,
            "æ¶¨è·Œ%": pct_chg,
            "è¯„åˆ†": score,
            "è¯„åˆ†(0-100)": round(_clamp(score / MAX_POS_SCORE, 0.0, 1.0) * 100.0, 2),
            "å…¥é€‰": False,
            "åŸå› ": "ä¸‹è¡ŒRSVåˆ†ä½è¿‡é«˜(è·³è¿‡)",
            "ç¼©é‡åˆ†ä½": round(v_rank, 2),
            "VRP": round(vrp_rank, 2),
            "ILLIQ": round(comp_illiq, 2),
            "ä¸‹è¡ŒRSV": round(float(down_ratio), 4) if pd.notna(down_ratio) else None,
            "ä¸‹è¡ŒRSVåˆ†ä½": round(float(down_rank), 2),
        }

    ofi_score, ofi_val = calculate_ofi_signal(open_, close, volume)
    if ofi_score > 0:
        score += ofi_score
        signals.append("OFI+")

    ma5 = close.rolling(5).mean()
    if len(ma5) > 2 and (current_price is not None) and np.isfinite(current_price):
        if float(current_price) > float(ma5.iloc[-1]) and float(ma5.iloc[-1]) > float(ma5.iloc[-2]):
            score += SCORE_CRITERIA["rebound_confirm"]
            signals.append("å¯åŠ¨")

    industry = None
    sector_ff = None
    hot_rank = None
    weibo_rate = None

    if score >= THRESHOLD_POTENTIAL - 2:
        if hot_rank_map:
            hot_rank = hot_rank_map.get(symbol)
            if hot_rank is not None and int(hot_rank) <= 20:
                score -= SCORE_CRITERIA["heat_penalty"]
                signals.append("çƒ­åº¦è¿‡é«˜")

        if weibo_rate_map and name:
            weibo_rate = weibo_rate_map.get(str(name))
            if weibo_rate is not None and np.isfinite(weibo_rate):
                if float(weibo_rate) <= -2.0:
                    score += SCORE_CRITERIA["weibo_panic"]
                    signals.append("èˆ†æƒ…åç©º")
                elif float(weibo_rate) >= 2.0:
                    score -= SCORE_CRITERIA["weibo_hype_penalty"]
                    signals.append("èˆ†æƒ…åçƒ­")

        if sector_fund_flow_map:
            industry = _get_industry(symbol)
            if industry is not None:
                sector_ff = sector_fund_flow_map.get(industry)
                if sector_ff is not None and np.isfinite(sector_ff):
                    if float(sector_ff) >= 0.5:
                        score += SCORE_CRITERIA["sector_fund_flow_strong"]
                        signals.append("æ¿å—å‡€æµå…¥å¼º")
                    elif float(sector_ff) > 0:
                        score += SCORE_CRITERIA["sector_fund_flow_ok"]
                        signals.append("æ¿å—å‡€æµå…¥")

    selected = bool(score >= THRESHOLD_POTENTIAL)

    score100 = round(_clamp(score / MAX_POS_SCORE, 0.0, 1.0) * 100.0, 2)

    return {
        "ä»£ç ": symbol,
        "åç§°": name,
        "ç°ä»·": current_price,
        "æ¶¨è·Œ%": pct_chg,
        "è¯„åˆ†": int(score),
        "è¯„åˆ†(0-100)": score100,
        "å…¥é€‰": selected,
        "æ¦œå•": "ä¸¥é€‰" if score >= THRESHOLD_HIGH_QUALITY else ("æ½œåŠ›" if selected else "-"),
        "è¡Œä¸š": industry,
        "æ¿å—5æ—¥ä¸»åŠ›å‡€å æ¯”": round(float(sector_ff), 2) if sector_ff is not None and np.isfinite(sector_ff) else None,
        "çƒ­åº¦æ’å": int(hot_rank) if hot_rank is not None else None,
        "å¾®åš24hçƒ­åº¦": round(float(weibo_rate), 2) if weibo_rate is not None and np.isfinite(weibo_rate) else None,
        "ç¼©é‡åˆ†ä½": round(float(v_rank), 2),
        "VRP": round(float(vrp_rank), 2),
        "ILLIQ": round(float(comp_illiq), 2),
        "ARåˆ†ä½": round(float(ar_rank), 2) if pd.notna(ar_rank) else None,
        "ä¸‹è¡ŒRSV": round(float(down_ratio), 4) if pd.notna(down_ratio) else None,
        "ä¸‹è¡ŒRSVåˆ†ä½": round(float(down_rank), 2) if pd.notna(down_rank) else None,
        "ä¿¡å·": "+".join(signals),
        "åŸå› ": prefilter_reason,
    }


rows = []
for c in codes:
    cs = "" if c is None else str(c).strip()
    if not cs:
        continue
    try:
        rows.append(_calc_one(cs))
    except Exception as e:
        symbol = _normalize_symbol(cs)
        rows.append({"ä»£ç ": symbol, "åç§°": _get_name(symbol), "å…¥é€‰": False, "åŸå› ": f"å¼‚å¸¸: {e}"})

df = pd.DataFrame(rows)
if not df.empty and "è¯„åˆ†(0-100)" in df.columns:
    df = df.sort_values(["è¯„åˆ†(0-100)"], ascending=False, na_position="last").reset_index(drop=True)

print("rows=", len(df))
```

## è¡Œä¸šèµ„é‡‘æµå‘

- idï¼š`3`
- is_pinnedï¼š`0`
- updated_atï¼š`2026-01-16 01:32:08`
- last_run_statusï¼š`pending`

### script_content

```python
# å¯¼å…¥å¿…è¦ä¾èµ–åº“
import akshare as ak
import pandas as pd
from datetime import datetime

# ====================== ç¬¬ä¸€æ­¥ï¼šæ—¥å¿—è¾“å‡ºï¼ˆåˆ†å±‚æ‰“å°ï¼Œç¬¦åˆLogè§„èŒƒï¼‰ ======================
print("=== è¡Œä¸šèµ„é‡‘æµå‘åˆ†æè„šæœ¬å¼€å§‹æ‰§è¡Œ ===")
print(f"æ‰§è¡Œæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"æ•°æ®æ¥å£ï¼šak.stock_sector_fund_flow_rank()ï¼ˆè¡Œä¸šèµ„é‡‘æµå‘æ’åï¼‰")
print("=" * 50)

# ====================== ç¬¬äºŒæ­¥ï¼šè·å–è¡Œä¸šèµ„é‡‘æµå‘åŸå§‹æ•°æ® ======================
# è·å–å…¨å¸‚åœºè¡Œä¸šèµ„é‡‘æµå‘æ•°æ®
sector_fund_flow_raw = ak.stock_sector_fund_flow_rank()

# æ‰“å°åŸå§‹æ•°æ®ä¿¡æ¯
print(f"åŸå§‹è¡Œä¸šæ•°æ®æ€»è¡Œæ•°ï¼š{len(sector_fund_flow_raw)}")
print(f"åŸå§‹æ•°æ®åˆ—åï¼š{list(sector_fund_flow_raw.columns)}")
print("=" * 50)

# ====================== ç¬¬ä¸‰æ­¥ï¼šæ•°æ®é¢„å¤„ç†ï¼ˆç¬¦åˆDataè¾“å‡ºè§„èŒƒï¼‰ ======================
# 1. å»é™¤ç©ºå€¼å’Œé‡å¤è®°å½•ï¼ˆæ•°æ®æ¸…æ´—ï¼‰
cleaned_sector_df = sector_fund_flow_raw.dropna().drop_duplicates()

# 2. æ§åˆ¶è¿”å›è¡Œæ•°ï¼ˆé¿å…å‰ç«¯å¡é¡¿ï¼Œå–å‰50æ¡ï¼‰
final_sector_df = cleaned_sector_df.head(50).reset_index(drop=True)

# 3. æ‰“å°é¢„å¤„ç†åä¿¡æ¯ï¼ˆLogåˆ†å±‚è¾“å‡ºè¦æ±‚ï¼‰
print(f"æ¸…æ´—åæ•°æ®è¡Œæ•°ï¼ˆå»ç©º/å»é‡ï¼‰ï¼š{len(cleaned_sector_df)}")
print(f"æœ€ç»ˆè¿”å›æ•°æ®è¡Œæ•°ï¼ˆå‰50æ¡ï¼‰ï¼š{len(final_sector_df)}")

# 4. å…³é”®ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸»åŠ›å‡€æµå…¥æ ¸å¿ƒæŒ‡æ ‡ï¼‰
if "ä¸»åŠ›å‡€æµå…¥" in final_sector_df.columns:
    main_inflow_max = final_sector_df["ä¸»åŠ›å‡€æµå…¥"].max()
    main_inflow_min = final_sector_df["ä¸»åŠ›å‡€æµå…¥"].min()
    main_inflow_mean = final_sector_df["ä¸»åŠ›å‡€æµå…¥"].mean()
    print(f"ä¸»åŠ›å‡€æµå…¥ç»Ÿè®¡ï¼ˆå•ä½ï¼šä¸‡å…ƒï¼‰ï¼š")
    print(f"  æœ€å¤§å€¼ï¼š{main_inflow_max:.2f}")
    print(f"  æœ€å°å€¼ï¼š{main_inflow_min:.2f}")
    print(f"  å¹³å‡å€¼ï¼š{main_inflow_mean:.2f}")
print("=" * 50)

# ç¬¬å››æ­¥ï¼šæ„å»ºCharté…ç½®ï¼ˆä¿®æ­£å­—æ®µåï¼Œä¸å®é™…æ•°æ®åˆ—åå®Œå…¨å¯¹é½ï¼‰
x_key = "åç§°"  # å®é™…æ•°æ®ä¸­â€œè¡Œä¸šåç§°â€å¯¹åº”çš„åˆ—æ˜¯â€œåç§°â€
series_key1 = "ä»Šæ—¥ä¸»åŠ›å‡€æµå…¥-å‡€é¢"  # å®é™…æ•°æ®ä¸­â€œä¸»åŠ›å‡€æµå…¥â€å¯¹åº”çš„åˆ—
series_key2 = "ä»Šæ—¥ä¸»åŠ›å‡€æµå…¥-å‡€å æ¯”"  # å®é™…æ•°æ®ä¸­â€œä¸»åŠ›å‡€å æ¯”â€å¯¹åº”çš„åˆ—

chart = {
    "xKey": x_key,
    "series": [
        {
            "key": series_key1,
            "type": "bar",
            "color": "#1890ff"
        },
        {
            "key": series_key2,
            "type": "line",
            "color": "#f5222d"
        }
    ]
}
print(f"å›¾è¡¨é…ç½®ç”ŸæˆæˆåŠŸï¼š")
print(f"  æ¨ªè½´å­—æ®µï¼š{x_key}")
print(f"  æ•°æ®ç³»åˆ—1ï¼š{series_key1}ï¼ˆæŸ±çŠ¶å›¾ï¼‰")
print(f"  æ•°æ®ç³»åˆ—2ï¼š{series_key2}ï¼ˆæŠ˜çº¿å›¾ï¼‰")

# ====================== ç¬¬äº”æ­¥ï¼šæœ€ç»ˆæ—¥å¿—æ€»ç»“ï¼ˆç¬¦åˆLogè§„èŒƒï¼‰ ======================
# æå–æ ¸å¿ƒç»“è®º
if "ä¸»åŠ›å‡€æµå…¥" in final_sector_df.columns and len(final_sector_df) > 0:
    top_sector = final_sector_df.loc[final_sector_df["ä¸»åŠ›å‡€æµå…¥"].idxmax(), "è¡Œä¸šåç§°"]
    top_inflow = final_sector_df["ä¸»åŠ›å‡€æµå…¥"].max()
    bottom_sector = final_sector_df.loc[final_sector_df["ä¸»åŠ›å‡€æµå…¥"].idxmin(), "è¡Œä¸šåç§°"]
    bottom_inflow = final_sector_df["ä¸»åŠ›å‡€æµå…¥"].min()

    print("=== è¡Œä¸šèµ„é‡‘æµå‘æ ¸å¿ƒç»“è®º ===")
    print(f"1. èµ„é‡‘æµå…¥æœ€å¤šè¡Œä¸šï¼š{top_sector}ï¼ˆ{top_inflow:.2f} ä¸‡å…ƒï¼‰")
    print(f"2. èµ„é‡‘æµå…¥æœ€å°‘è¡Œä¸šï¼š{bottom_sector}ï¼ˆ{bottom_inflow:.2f} ä¸‡å…ƒï¼‰")
    print(f"3. å¯è§†åŒ–å›¾è¡¨ï¼šå·²ç”Ÿæˆï¼ˆChartæ ‡ç­¾é¡µæŸ¥çœ‹ï¼‰")
    print(f"4. è¯¦ç»†æ•°æ®ï¼š{len(final_sector_df)} ä¸ªè¡Œä¸šï¼ˆDataæ ‡ç­¾é¡µæŸ¥çœ‹ï¼‰")
print("=== è¡Œä¸šèµ„é‡‘æµå‘åˆ†æè„šæœ¬æ‰§è¡Œå®Œæˆ ===")

# ====================== è¾“å‡ºæœ€ç»ˆæ•°æ®ï¼ˆç¬¦åˆDataè¾“å‡ºå¥‘çº¦ï¼Œdfä½œä¸ºæ ¸å¿ƒæ•°æ®è½½ä½“ï¼‰ ======================
df = final_sector_df
```

## å¯»æ‰¾ä½ä¼°è‚¡ç¥¨

- idï¼š`13`
- is_pinnedï¼š`0`
- updated_atï¼š`2026-01-09 02:44:00`
- last_run_statusï¼š`pending`

### script_content

```python
import os
import sys

# Ensure backend is in path to import pymr_compat
# backend is current dir if run from backend, but let's be safe
if "backend" not in sys.path:
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    import pymr_compat
    pymr_compat.ensure_py_mini_racer()
except ImportError:
    pass

import streamlit as st
import pandas as pd
import numpy as np
import akshare as ak
import datetime
import time
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# ==========================================
# æ ¸å¿ƒè§„èŒƒï¼šè„šæœ¬é…ç½®ä¸åˆå§‹åŒ–
# ==========================================
st.set_page_config(page_title="è¡¥æ¶¨æ½œä¼é‡åŒ–ç­›é€‰å™¨ v1.1", layout="wide")

# è‡ªå®šä¹‰æ—¥å¿—å‡½æ•°ï¼šéµå¾ªåè®®
def append_log(msg, level="info"):
    icon = {"info": "ğŸš€", "success": "âœ…", "error": "âŒ"}.get(level, "ğŸ“")
    if "logs" not in st.session_state:
        st.session_state.logs = []
    st.session_state.logs.append(f"{icon} [{datetime.datetime.now().strftime('%H:%M:%S')}] {msg}")

# ==========================================
# æ•°æ®ç¼“å­˜å±‚ï¼šåˆ©ç”¨ Streamlit ç¼“å­˜å‡å°‘ API è°ƒç”¨é¢‘ç‡
# ==========================================

@st.cache_data(ttl=3600)  # ç¼“å­˜1å°æ—¶
def get_industry_list():
    """è·å–æ‰€æœ‰è¡Œä¸šæ¿å—"""
    try:
        return ak.stock_board_industry_name_em()
    except Exception as e:
        return pd.DataFrame()

@st.cache_data(ttl=600)  # ç¼“å­˜10åˆ†é’Ÿ
def get_sector_cons(sector_name):
    """è·å–æ¿å—æˆåˆ†è‚¡"""
    try:
        return ak.stock_board_industry_cons_em(symbol=sector_name)
    except Exception as e:
        return pd.DataFrame()

# ==========================================
# æ ¸å¿ƒé€»è¾‘åŒºï¼šè¡¥æ¶¨æ‰«æå¼•æ“
# ==========================================

class CatchUpScanner:
    def __init__(self, bias_limit, vol_min):
        self.bias_limit = bias_limit
        self.vol_min = vol_min

    def get_stock_metrics(self, symbol, name):
        """æ ¸å¿ƒæŒ‡æ ‡è®¡ç®— (Alpha#2 + æ¨ªæˆªé¢æ€è·¯)"""
        try:
            # è·å–æ—¥çº¿è¡Œæƒ…ï¼Œé»˜è®¤åå¤æƒ
            df = ak.stock_zh_a_hist(symbol=symbol, period="daily", adjust="qfq")
            if df is None or len(df) < 65:
                return None
            
            close = df['æ”¶ç›˜']
            volume = df['æˆäº¤é‡']
            
            # 1. 5æ—¥æ”¶ç›Šç‡
            ret_5d = (close.iloc[-1] / close.iloc[-6] - 1) * 100
            
            # 2. MA60 åç¦»åº¦ (Bias)
            ma60 = close.rolling(60).mean().iloc[-1]
            bias_60 = (close.iloc[-1] / ma60 - 1) * 100
            
            # 3. é‡æ¯” (å¼‚åŠ¨åˆ†æ)
            avg_vol_5d = volume.iloc[-6:-1].mean()
            vol_ratio = volume.iloc[-1] / avg_vol_5d if avg_vol_5d > 0 else 0
            
            return {
                "ä»£ç ": symbol,
                "åç§°": name,
                "ç°ä»·": close.iloc[-1],
                "5æ—¥æ¶¨å¹…%": round(ret_5d, 2),
                "MA60åç¦»%": round(bias_60, 2),
                "é‡æ¯”": round(vol_ratio, 2),
                "raw_data": df.tail(60) # æš‚å­˜ç”¨äºç»˜å›¾
            }
        except:
            return None

# ==========================================
# é¡µé¢å¸ƒå±€è®¾è®¡
# ==========================================

st.title("ğŸ¹ è¡¥æ¶¨æ½œä¼é‡åŒ–ç­›é€‰å™¨ (v1.1)")
st.caption(f"å½“å‰è¿è¡Œç¯å¢ƒ: akshare v1.17.96 | Python 3.9")

# ä¾§è¾¹æ ï¼šå‚æ•°åŒº
with st.sidebar:
    st.header("âš™ï¸ æ‰«æå‚æ•°")
    target_sector_count = st.number_input("å±•ç¤ºçƒ­é—¨æ¿å—æ•°é‡", 5, 50, 20)
    
    st.divider()
    st.markdown("### ç­›é€‰é˜ˆå€¼")
    bias_th = st.slider("æœ€å¤§MA60åç¦» (å®‰å…¨å«)", 0.0, 20.0, 8.0, help="è‚¡ä»·è·ç¦»60æ—¥å‡çº¿è¶Šè¿‘ï¼Œè¡¥æ¶¨çš„ç¡®å®šæ€§ç›¸å¯¹è¶Šé«˜")
    vol_th = st.slider("æœ€ä½é‡æ¯” (å¼‚åŠ¨åº¦)", 0.5, 3.0, 1.2, help="é‡æ¯”>1.2è¡¨ç¤ºä»Šæ—¥èµ„é‡‘æœ‰æ˜æ˜¾æµå…¥è¿¹è±¡")
    
    if st.button("æ¸…é™¤ç¼“å­˜"):
        st.cache_data.clear()
        st.rerun()

# ä¸»ç•Œé¢ï¼šé€»è¾‘æµ
df_all_sectors = get_industry_list()

if not df_all_sectors.empty:
    sector_options = df_all_sectors.head(target_sector_count)['æ¿å—åç§°'].tolist()
    selected_sector = st.selectbox("ğŸ¯ é€‰æ‹©é¢†æ¶¨æ¿å—è¿›è¡Œâ€˜æŒ–æ˜è¡¥æ¶¨â€™ï¼š", sector_options)
    
    if st.button("å¼€å§‹æ·±åº¦æ‰«æ", type="primary"):
        st.session_state.logs = [] # é‡ç½®æ—¥å¿—
        append_log(f"é”å®šæ¿å—: {selected_sector}", "info")
        
        df_cons = get_sector_cons(selected_sector)
        if not df_cons.empty:
            total_stocks = len(df_cons)
            append_log(f"å…±å‘ç° {total_stocks} åªæˆåˆ†è‚¡ï¼Œå¼€å§‹è®¡ç®—é‡ä»·å…±æŒ¯æŒ‡æ ‡...", "info")
            
            scanner = CatchUpScanner(bias_th, vol_th)
            results = []
            
            # è¿›åº¦å±•ç¤º
            progress_text = st.empty()
            bar = st.progress(0)
            
            start_time = time.time()
            
            for i, row in df_cons.iterrows():
                stock_res = scanner.get_stock_metrics(row['ä»£ç '], row['åç§°'])
                if stock_res:
                    results.append(stock_res)
                
                # æ›´æ–°è¿›åº¦æ¡
                p = (i + 1) / total_stocks
                bar.progress(p)
                progress_text.text(f"æ­£åœ¨å¤„ç†: {row['åç§°']} ({i+1}/{total_stocks})")
            
            # æ ¸å¿ƒè®¡ç®—ï¼šè®¡ç®—æ¿å—å¹³å‡5æ—¥æ”¶ç›Š
            df_res = pd.DataFrame(results)
            if not df_res.empty:
                sector_avg_ret = df_res['5æ—¥æ¶¨å¹…%'].mean()
                
                # è¡¥æ¶¨ç­›é€‰é€»è¾‘
                # 1. 5æ—¥æ¶¨å¹…ä½äºæ¿å—å‡å€¼ (è¡¥æ¶¨ä½)
                # 2. åç¦»åº¦åœ¨è®¾å®šçš„å®‰å…¨çº¿å†…
                # 3. é‡æ¯”è¾¾åˆ°å¼‚åŠ¨é˜ˆå€¼
                df_targets = df_res[
                    (df_res['5æ—¥æ¶¨å¹…%'] < sector_avg_ret) & 
                    (df_res['MA60åç¦»%'] < bias_th) & 
                    (df_res['é‡æ¯”'] > vol_th)
                ].copy()
                
                elapsed = time.time() - start_time
                append_log(f"æ‰«æå®Œæˆï¼Œè€—æ—¶ {elapsed:.1f}s", "success")

                # ==========================================
                # ç»“æœè¾“å‡ºåŒº
                # ==========================================
                col1, col2 = st.columns([2, 3])
                
                with col1:
                    st.metric("æ¿å—å¹³å‡5æ—¥æ¶¨å¹…", f"{sector_avg_ret:.2f}%")
                    st.subheader("ğŸ“‹ è¡¥æ¶¨æ½œåŠ›æ¸…å•")
                    if not df_targets.empty:
                        # æŒ‰é‡æ¯”é™åºæ’åº
                        df_display = df_targets.sort_values("é‡æ¯”", ascending=False)
                        st.dataframe(
                            df_display[["ä»£ç ", "åç§°", "5æ—¥æ¶¨å¹…%", "MA60åç¦»%", "é‡æ¯”"]],
                            hide_index=True,
                            use_container_width=True
                        )
                    else:
                        st.warning("æ— ç¬¦åˆä¸¥æ ¼è¡¥æ¶¨é€»è¾‘çš„æ ‡çš„ï¼Œå¯å°è¯•è°ƒé«˜MA60åç¦»é˜ˆå€¼ã€‚")
                
                with col2:
                    if not df_targets.empty:
                        top_pick = df_targets.sort_values("é‡æ¯”", ascending=False).iloc[0]
                        st.subheader(f"ğŸ” å¼‚åŠ¨ç„¦ç‚¹: {top_pick['åç§°']}")
                        
                        # ç»˜åˆ¶Kçº¿å›¾
                        hist = top_pick['raw_data']
                        fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.05, row_heights=[0.7, 0.3])
                        fig.add_trace(go.Candlestick(x=hist['æ—¥æœŸ'], open=hist['å¼€ç›˜'], high=hist['æœ€é«˜'], 
                                                    low=hist['æœ€ä½'], close=hist['æ”¶ç›˜'], name="Kçº¿"), row=1, col=1)
                        fig.add_trace(go.Bar(x=hist['æ—¥æœŸ'], y=hist['æˆäº¤é‡'], name="æˆäº¤é‡", marker_color='rgba(0,100,250,0.5)'), row=2, col=1)
                        
                        fig.update_layout(height=500, margin=dict(t=30, b=10, l=10, r=10), xaxis_rangeslider_visible=False)
                        st.plotly_chart(fig, use_container_width=True)
            else:
                append_log("æœªèƒ½è·å–åˆ°ä»»ä½•æœ‰æ•ˆçš„è¡Œæƒ…æ•°æ®", "error")
        else:
            append_log("è·å–æ¿å—æˆåˆ†è‚¡å¤±è´¥", "error")

# æ—¥å¿—è¾“å‡ºåŒº (åº•éƒ¨å±•å¼€)
with st.expander("ğŸ› ï¸ ç³»ç»Ÿæ—¥å¿— (Log)", expanded=True):
    if "logs" in st.session_state:
        for log in st.session_state.logs[::-1]:
            st.write(log)

st.divider()
st.markdown("âš ï¸ **é£é™©æç¤º**ï¼šæœ¬è„šæœ¬åŸºäºè¡¥æ¶¨é€»è¾‘ä¸é‡ä»·å¼‚åŠ¨ç­›é€‰ï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚è¡¥æ¶¨è‚¡èƒœç‡å—åˆ¶äºå¤§ç›˜ç³»ç»Ÿæ€§é£é™©åŠé¾™å¤´è‚¡çš„æŒç»­æ€§ã€‚")
```

## PE_band

- idï¼š`2`
- is_pinnedï¼š`0`
- updated_atï¼š`2026-01-08 03:51:16`
- last_run_statusï¼š`pending`

### script_content

```python
import streamlit as st
import pandas as pd
import numpy as np
import akshare as ak
import datetime

# ==========================================
# 0. é¡µé¢é…ç½®ä¸å…¨å±€è®¾ç½®
# ==========================================
st.set_page_config(
    page_title="ä¸ªè‚¡ PE-Band ä¼°å€¼åˆ†æå·¥å…·",
    page_icon="ğŸ“ˆ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# éšè—éƒ¨åˆ†é»˜è®¤æ ·å¼
hide_streamlit_style = """
<style>
#MainMenu {visibility: hidden;}
footer {visibility: hidden;}
</style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# ==========================================
# 1. æ ¸å¿ƒé€»è¾‘å‡½æ•°å°è£… (å¸¦ç¼“å­˜)
# ==========================================

@st.cache_data(ttl=3600)  # è®¾ç½®ç¼“å­˜æœ‰æ•ˆæœŸä¸º1å°æ—¶
def get_stock_price(symbol, lookback_days):
    """
    [Data Fetch] è·å–æ—¥çº¿è¡Œæƒ…
    """
    end_date = datetime.datetime.now().strftime("%Y%m%d")
    # å¤šæ‹‰å–ä¸€å¹´æ•°æ®ï¼Œç¡®ä¿å¼€å¤´æœ‰è´¢åŠ¡æ•°æ®è¦†ç›–
    start_date = (datetime.datetime.now() - datetime.timedelta(days=lookback_days + 365)).strftime("%Y%m%d")
    
    try:
        df_price = ak.stock_zh_a_hist(symbol=symbol, start_date=start_date, end_date=end_date, adjust="qfq")
        if df_price is None or df_price.empty:
            return None
        
        df_price = df_price[['æ—¥æœŸ', 'æ”¶ç›˜']].rename(columns={'æ—¥æœŸ': 'date', 'æ”¶ç›˜': 'close'})
        df_price['date'] = pd.to_datetime(df_price['date'])
        df_price = df_price.sort_values('date')
        return df_price
    except Exception as e:
        st.error(f"è¡Œæƒ…æ•°æ®è·å–å¤±è´¥: {e}")
        return None

@st.cache_data(ttl=3600)
def get_financial_data(symbol):
    """
    [Data Fetch] è·å–å¹¶æ¸…æ´—è´¢åŠ¡EPSæ•°æ® (TTM + å¼‚å¸¸å€¼å¤„ç†)
    """
    try:
        df_abstract = ak.stock_financial_abstract(symbol=symbol)
        if df_abstract is None or df_abstract.empty:
            return None, "æœªæ‰¾åˆ°è´¢åŠ¡æ•°æ®"

        # 1. æ¨¡ç³ŠåŒ¹é…å¯»æ‰¾ EPS è¡Œ
        df_abstract['æŒ‡æ ‡'] = df_abstract['æŒ‡æ ‡'].astype(str)
        target_keywords = ["åŸºæœ¬æ¯è‚¡æ”¶ç›Š", "æ¯è‚¡æ”¶ç›Š(åŸºæœ¬)", "æ¯è‚¡æ”¶ç›Š", "å½’å±æ¯å…¬å¸è‚¡ä¸œçš„å‡€åˆ©æ¶¦"]
        
        target_row = None
        row_name = ""
        for kw in target_keywords:
            mask = df_abstract['æŒ‡æ ‡'].str.contains(kw)
            if mask.any():
                target_row = df_abstract[mask].iloc[0]
                row_name = kw
                break
        
        if target_row is None:
            return None, "æœªæ‰¾åˆ°EPSç›¸å…³æŒ‡æ ‡"

        # 2. TTM å¹´åŒ–å¤„ç†
        date_cols = [c for c in df_abstract.columns if c.isdigit() and len(c) == 8]
        eps_records = []
        
        for d_col in date_cols:
            try:
                dt = pd.to_datetime(d_col, format='%Y%m%d')
                val = float(target_row[d_col])
                
                # --- TTM å¹´åŒ–ç®—æ³• ---
                month = dt.month
                annual_eps = val 
                if month == 3: annual_eps = val * 4
                elif month == 6: annual_eps = val * 2
                elif month == 9: annual_eps = val / 3 * 4
                
                if annual_eps > 0.001:
                    eps_records.append({'date': dt, 'eps': annual_eps})
            except:
                continue
        
        df_fin = pd.DataFrame(eps_records).sort_values(by='date')

        # 3. å¼‚å¸¸å€¼å‰”é™¤ (3-Sigma)
        if len(df_fin) > 8:
            mean_eps = df_fin['eps'].mean()
            std_eps = df_fin['eps'].std()
            upper = mean_eps + 3 * std_eps
            lower = mean_eps - 3 * std_eps
            df_fin = df_fin[(df_fin['eps'] <= upper) & (df_fin['eps'] >= lower)]
            
        return df_fin, row_name
        
    except Exception as e:
        st.error(f"è´¢åŠ¡æ•°æ®è§£æå¤±è´¥: {e}")
        return None, str(e)

def calculate_pe_band(df_price, df_fin, pe_list, lookback_days):
    """
    [Core Calc] åˆå¹¶æ•°æ®å¹¶è®¡ç®—PEé€šé“
    """
    # Merge Asof
    df_merge = pd.merge_asof(df_price, df_fin, on='date', direction='backward')
    df_merge['eps'] = df_merge['eps'].ffill()
    df_merge = df_merge.dropna(subset=['eps'])
    
    # Calculate Bands
    for pe in pe_list:
        df_merge[f"PE {pe}x"] = df_merge['eps'] * pe
        
    # Crop Data
    df_final = df_merge.tail(lookback_days).copy()
    return df_final

# ==========================================
# 2. UI å¸ƒå±€ä¸äº¤äº’é€»è¾‘
# ==========================================

# --- Sidebar: å‚æ•°è®¾ç½®åŒº ---
with st.sidebar:
    st.header("âš™ï¸ å‚æ•°é…ç½®")
    
    input_symbol = st.text_input("è‚¡ç¥¨ä»£ç  (Symbol)", value="002371", help="è¾“å…¥Aè‚¡ä»£ç ï¼Œå¦‚ 600519 æˆ– 002371")
    
    st.subheader("ä¼°å€¼é€šé“è®¾ç½®")
    pe1 = st.number_input("ä½ä¼°çº¿ (Low PE)", value=20, step=1)
    pe2 = st.number_input("ä¸­æ¢çº¿ (Mid PE)", value=30, step=1)
    pe3 = st.number_input("é«˜ä¼°çº¿ (High PE)", value=40, step=1)
    target_pe_list = [pe1, pe2, pe3]
    
    lookback = st.slider("å›æº¯å¤©æ•° (Lookback)", min_value=100, max_value=2000, value=500, step=100)
    
    run_btn = st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary")

# --- Main: ä¸»ç•Œé¢é€»è¾‘ ---
st.title(f"ğŸ“Š Aè‚¡æ·±åº¦ä¼°å€¼åˆ†æå·¥å…·")
st.caption("æ•°æ®æ¥æº: AkShareå¼€æºæ¥å£ | æ¨¡å‹: TTMåŠ¨æ€å¸‚ç›ˆç‡ + 3-Sigmaæ¸…æ´—")

if run_btn:
    with st.spinner(f"æ­£åœ¨æ‹‰å– {input_symbol} çš„æ•°æ®ï¼Œè¯·ç¨å€™..."):
        # 1. è·å–æ•°æ®
        df_price = get_stock_price(input_symbol, lookback)
        df_fin, idx_name = get_financial_data(input_symbol)
        
        if df_price is not None and df_fin is not None:
            # 2. æ ¸å¿ƒè®¡ç®—
            df_result = calculate_pe_band(df_price, df_fin, target_pe_list, lookback)
            
            if df_result.empty:
                st.warning("âš ï¸ è®¡ç®—ç»“æœä¸ºç©ºï¼Œè¯·æ£€æŸ¥è‚¡ç¥¨ä»£ç æˆ–è°ƒæ•´å›æº¯æ—¶é—´ã€‚")
            else:
                # 3.1 æ ¸å¿ƒæŒ‡æ ‡çœ‹æ¿ (Metrics)
                latest = df_result.iloc[-1]
                curr_price = latest['close']
                curr_eps = latest['eps']
                curr_pe = curr_price / curr_eps
                
                # ä¼°å€¼çŠ¶æ€åˆ¤å®š
                if curr_pe < target_pe_list[0]:
                    status = "ğŸŸ¢ æåº¦ä½ä¼°"
                    delta_color = "normal" 
                elif curr_pe < target_pe_list[1]:
                    status = "ğŸŸ¡ ç›¸å¯¹ä½ä¼°"
                    delta_color = "off"
                elif curr_pe < target_pe_list[2]:
                    status = "ğŸŸ  ç›¸å¯¹é«˜ä¼°"
                    delta_color = "inverse"
                else:
                    status = "ğŸ”´ æåº¦é«˜ä¼°"
                    delta_color = "inverse"

                st.markdown("### ğŸ“Œ æ ¸å¿ƒæŒ‡æ ‡æ‘˜è¦")
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("æœ€æ–°æ”¶ç›˜ä»·", f"Â¥{curr_price:.2f}")
                col2.metric("å½“å‰ TTM PE", f"{curr_pe:.2f} x", delta=f"{curr_pe - target_pe_list[1]:.1f} (vs ä¸­æ¢)", delta_color="inverse")
                col3.metric("å¹´åŒ– EPS", f"Â¥{curr_eps:.2f}", help=f"åŸºäºæŒ‡æ ‡: {idx_name}")
                col4.metric("ä¼°å€¼çŠ¶æ€", status)
                
                st.divider()

                # 3.2 äº¤äº’å¼å›¾è¡¨ (Chart)
                st.markdown(f"### ğŸ“ˆ PE-Band èµ°åŠ¿å›¾ ({input_symbol})")
                
                # æ•´ç†ç»˜å›¾æ•°æ®ï¼šå°† date è®¾ä¸ºç´¢å¼•ï¼Œåªä¿ç•™éœ€è¦ç»˜åˆ¶çš„åˆ—
                chart_cols = ['close'] + [f"PE {pe}x" for pe in target_pe_list]
                chart_data = df_result.set_index('date')[chart_cols]
                
                # ä½¿ç”¨ Streamlit åŸç”Ÿå›¾è¡¨ (ç®€å•ã€ç¾è§‚)
                st.line_chart(
                    chart_data,
                    color=["#1890ff", "#52c41a", "#faad14", "#f5222d"], # è“(è‚¡ä»·), ç»¿(ä½), é»„(ä¸­), çº¢(é«˜)
                    use_container_width=True,
                    height=500
                )
                
                # 3.3 è¯¦ç»†æ•°æ®å±•ç¤º (Data)
                with st.expander("ğŸ” æŸ¥çœ‹è¯¦ç»†å†å²æ•°æ® (Data Table)"):
                    st.dataframe(
                        df_result.style.format({
                            "close": "{:.2f}", 
                            "eps": "{:.4f}",
                            f"PE {pe1}x": "{:.2f}",
                            f"PE {pe2}x": "{:.2f}",
                            f"PE {pe3}x": "{:.2f}"
                        }),
                        use_container_width=True
                    )
                
                # 3.4 æ™ºèƒ½è¯„è¯­ (Log)
                st.info(f"""
                **ğŸ’¡ æ™ºèƒ½åˆ†ææŠ¥å‘Š**:
                å½“å‰ **{input_symbol}** çš„è‚¡ä»·ä¸º **{curr_price}** å…ƒï¼Œå¯¹åº”çš„åŠ¨æ€å¸‚ç›ˆç‡ä¸º **{curr_pe:.2f}** å€ã€‚
                ç›¸è¾ƒäºè®¾å®šçš„ä¼°å€¼ä¸­æ¢ (**{target_pe_list[1]}å€ PE**)ï¼Œå½“å‰å¤„äº **{status}** åŒºåŸŸã€‚
                
                *æ³¨ï¼šEPSæ•°æ®å·²å‰”é™¤3-Sigmaæç«¯å¼‚å¸¸å€¼ï¼Œå¹¶åŸºäºæœ€æ–°è´¢æŠ¥è¿›è¡ŒTTMå¹´åŒ–å¤„ç†ã€‚*
                """)
                
        else:
            st.error("æ•°æ®æ‹‰å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥è‚¡ç¥¨ä»£ç æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ç¨åé‡è¯•ã€‚")

else:
    # åˆå§‹å¼•å¯¼é¡µ
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§è¾¹æ è¾“å…¥å‚æ•°ï¼Œå¹¶ç‚¹å‡»ã€å¼€å§‹åˆ†æã€‘æŒ‰é’®")
```

## è·å–æ¿å—æ–°é—»é›†åˆ

- idï¼š`10`
- is_pinnedï¼š`0`
- updated_atï¼š`2026-01-06 08:30:01`
- last_run_statusï¼š`pending`

### script_content

```python
import akshare as ak
import pandas as pd
import time
import random

# ==========================================
# 1. å‚æ•°è®¾ç½®åŒº (Parameter Setting)
# ==========================================
# æ¯åªè‚¡ç¥¨ä¿ç•™çš„æœ€æ–°æ–°é—»æ¡æ•°
news_limit_per_stock = 1

# ç›®æ ‡è¡Œä¸šæ¿å—åç§° (å¦‚ "åŠå¯¼ä½“", "é“¶è¡Œ", "ç™½é…’")
# è‹¥ä¸ºç©ºå­—ç¬¦ä¸² ""ï¼Œåˆ™é»˜è®¤è·å–å…¨å¸‚åœºæˆäº¤é¢æœ€é«˜çš„è‚¡ç¥¨
target_board = "è¯åˆ¸"

# æ¼”ç¤ºæ¨¡å¼ï¼šä»…å¤„ç†æˆäº¤é‡å‰ N çš„è‚¡ç¥¨ (é˜²æ­¢å…¨å¸‚åœºæˆ–æ¿å—å†…è‚¡ç¥¨è¿‡å¤šéå†è€—æ—¶è¿‡é•¿)
# è‹¥éœ€å…¨å¸‚åœºï¼Œè¯·å°†æ­¤æ•°å­—è°ƒå¤§ï¼Œä½†éœ€æ³¨æ„æ¥å£åçˆ¬é™åˆ¶
max_stocks_to_process = 100

print(f"ğŸš€ [Start] å¼€å§‹æ‰§è¡Œ Aè‚¡æ‰¹é‡æ–°é—»è·å–è„šæœ¬...")
if target_board:
    print(f"   æ¨¡å¼: æ¿å—æ¨¡å¼ - [{target_board}]")
else:
    print(f"   æ¨¡å¼: å…¨å¸‚åœºçƒ­é—¨æ¨¡å¼")
print(f"   é…ç½®: æ¯åªè‚¡ç¥¨ä¿ç•™æœ€æ–° {news_limit_per_stock} æ¡æ–°é—»")
print(f"   èŒƒå›´: ä»…å¤„ç†æ´»è·ƒåº¦å‰ {max_stocks_to_process} åªè‚¡ç¥¨")

# åˆå§‹åŒ–è¾“å‡ºå˜é‡ (éµå¾ª V2.0 åè®®)
df = pd.DataFrame()
chart = {}

try:
    # ==========================================
    # 2. æ ¸å¿ƒé€»è¾‘åŒº (Core Logic)
    # ==========================================

    # Step 1: è·å–ç›®æ ‡è‚¡ç¥¨æ± 
    if target_board:
        # æ¿å—æ¨¡å¼ï¼šè·å–æŒ‡å®šè¡Œä¸šæ¿å—çš„æˆåˆ†è‚¡
        print(f"Step 1: æ­£åœ¨è°ƒç”¨æ¿å—æ¥å£è·å– [{target_board}] æˆåˆ†è‚¡...")
        try:
            df_market = ak.stock_board_industry_cons_em(symbol=target_board)
        except Exception as e:
            raise ValueError(f"è·å–æ¿å—æ•°æ®å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¿å—åç§° '{target_board}' æ˜¯å¦æ­£ç¡®ã€‚é”™è¯¯: {e}")
    else:
        # å…¨å¸‚åœºæ¨¡å¼ï¼šè·å–å…¨å¸‚åœºå®æ—¶è¡Œæƒ…
        print("Step 1: æ­£åœ¨è·å–å½“å‰çƒ­é—¨è‚¡ç¥¨åˆ—è¡¨ä½œä¸ºç›®æ ‡æ± ...")
        df_market = ak.stock_zh_a_spot_em()
    
    # å¼‚å¸¸æ ¡éªŒ
    if df_market is None or df_market.empty:
        raise ValueError("è¡Œæƒ…/æ¿å—æ•°æ®è·å–ä¸ºç©º")

    # æ’åºå¹¶æˆªå–ï¼Œç¡®ä¿æœ‰æ•°æ®ä¸”æ˜¯æ´»è·ƒè‚¡
    # æ— è®ºæ˜¯æ¿å—è¿˜æ˜¯å…¨å¸‚åœºï¼Œéƒ½æŒ‰æˆäº¤é¢æ’åºï¼Œä¼˜å…ˆå…³æ³¨æ´»è·ƒè‚¡
    if 'æˆäº¤é¢' in df_market.columns:
        df_target = df_market.sort_values(by='æˆäº¤é¢', ascending=False).head(max_stocks_to_process)
    else:
        # é˜²å¾¡æ€§é€»è¾‘ï¼šå¦‚æœæ¥å£è¿”å›åˆ—åå˜åŠ¨ï¼Œå°è¯•æŒ‰é»˜è®¤é¡ºåºæˆªå–
        print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°'æˆäº¤é¢'åˆ—ï¼Œå°†æŒ‰é»˜è®¤é¡ºåºæˆªå–")
        df_target = df_market.head(max_stocks_to_process)

    stock_list = df_target['ä»£ç '].tolist()
    
    print(f"âœ… é”å®šç›®æ ‡è‚¡ç¥¨ {len(stock_list)} åª: {stock_list}")

    # Step 2: å¾ªç¯è·å–æ¯åªè‚¡ç¥¨çš„æ–°é—»
    all_news_list = []
    
    # å­—æ®µæ˜ å°„å­—å…¸ (æ ‡å‡†åŒ–è¾“å‡º)
    column_mapping = {
        'å…³é”®è¯': 'keyword',
        'æ–°é—»æ ‡é¢˜': 'title',
        'æ–°é—»å†…å®¹': 'content',
        'å‘å¸ƒæ—¶é—´': 'publish_time',
        'æ–‡ç« æ¥æº': 'source',
        'æ–°é—»é“¾æ¥': 'url'
    }

    print("Step 2: å¼€å§‹éå†è·å–æ–°é—» (è¯·è€å¿ƒç­‰å¾…)...")
    
    for i, symbol in enumerate(stock_list):
        try:
            # æ‰“å°è¿›åº¦ (ç§»é™¤ end="" å‚æ•°ä»¥å…¼å®¹æ‰€æœ‰ç¯å¢ƒ)
            print(f"   [{i+1}/{len(stock_list)}] æ­£åœ¨å¤„ç†: {symbol} ...")
            
            # è°ƒç”¨ä¸ªè‚¡æ–°é—»æ¥å£
            df_temp = ak.stock_news_em(symbol=symbol)
            
            if df_temp is None or df_temp.empty:
                print(f"      âš ï¸ {symbol} æ— æ–°é—»æ•°æ®")
                continue

            # 2.1 æ•°æ®æ¸…æ´—
            # é‡å‘½ååˆ—
            df_temp = df_temp.rename(columns=column_mapping)
            
            # ç¡®ä¿åªæœ‰éœ€è¦çš„åˆ—
            target_cols = ['title', 'source', 'publish_time', 'url']
            available_cols = [c for c in target_cols if c in df_temp.columns]
            df_clean = df_temp[available_cols].copy()
            
            # å¢åŠ è‚¡ç¥¨ä»£ç åˆ—ï¼ŒåŒºåˆ†å½’å±
            df_clean['symbol'] = symbol
            
            # 2.2 æ ¸å¿ƒæˆªæ–­é€»è¾‘ (Keep Top N)
            # ç¡®ä¿æŒ‰æ—¶é—´é™åº
            if 'publish_time' in df_clean.columns:
                df_clean = df_clean.sort_values(by='publish_time', ascending=False)
            
            # æˆªå–å‰ N æ¡
            df_final_slice = df_clean.head(news_limit_per_stock)
            
            # æ·»åŠ åˆ°æ€»åˆ—è¡¨
            all_news_list.append(df_final_slice)
            
            print(f"      âœ… æˆåŠŸè·å– {len(df_final_slice)} æ¡")
            
            # ç¤¼è²Œæ€§å»¶æ—¶ï¼Œé˜²æ­¢æ¥å£å°ç¦
            time.sleep(random.uniform(0.5, 1.5))
            
        except Exception as inner_e:
            print(f"      âŒ å‡ºé”™: {inner_e}")
            continue

    # Step 3: èšåˆæ•°æ®
    print("Step 3: èšåˆæ‰€æœ‰æ•°æ®...")
    if all_news_list:
        df_result = pd.concat(all_news_list, ignore_index=True)
    else:
        # å¦‚æœå®Œå…¨æ²¡æœ‰è·å–åˆ°æ•°æ®ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„æ ‡å‡†ç»“æ„
        df_result = pd.DataFrame(columns=['symbol', 'title', 'source', 'publish_time', 'url'])

    print(f"âœ… æ•°æ®å¤„ç†å®Œæˆï¼Œå…±æ”¶é›† {len(df_result)} æ¡èšåˆæ–°é—»")

    # ==========================================
    # 4. è¾“å‡ºæ„å»ºåŒº (Output Construction)
    # ==========================================
    
    # 4.1 å‡†å¤‡ Data
    df = df_result
    
    # 4.2 å‡†å¤‡ Chart
    # è¿™æ˜¯ä¸€ä¸ªçº¯æ•°æ®åˆ—è¡¨è„šæœ¬ï¼Œä¸éœ€è¦å›¾è¡¨ï¼Œè¿”å›ç©ºé…ç½®
    chart = {}
    
    # ç®€å•æ‰“å°ç»“æœé¢„è§ˆ
    if not df.empty:
        print("\n=== ç»“æœé¢„è§ˆ (Top 5) ===")
        print(df[['symbol', 'publish_time', 'title']].head(5).to_string(index=False))

except Exception as e:
    # ==========================================
    # 3. å¼‚å¸¸å¤„ç†åŒº (Exception Handling)
    # ==========================================
    print(f"âŒ [Error] å…¨å±€æ‰§è¡Œå‡ºé”™: {e}")
    df = pd.DataFrame()
    chart = {}
```

## ä¸ªè‚¡æ–°é—»è·å–

- idï¼š`9`
- is_pinnedï¼š`0`
- updated_atï¼š`2026-01-06 03:30:34`
- last_run_statusï¼š`pending`

### script_content

```python
import akshare as ak
import pandas as pd
import datetime

# ==========================================
# 1. å‚æ•°è®¾ç½®åŒº (Parameter Setting)
# ==========================================
symbol = "603777"  # ç›®æ ‡è‚¡ç¥¨ä»£ç  (ä¾‹å¦‚ï¼šåéŸ¡ç§‘æŠ€/ä»»æ„Aè‚¡ä»£ç )
limit_count = 100  # è·å–æ¡æ•°é™åˆ¶ (æ¥å£é»˜è®¤çº¦ä¸º100æ¡)

print(f"ğŸš€ [Start] å¼€å§‹è·å–ä¸ªè‚¡ {symbol} çš„æ–°é—»èµ„è®¯...")

# åˆå§‹åŒ–è¾“å‡ºå˜é‡ (éµå¾ª V2.0 åè®®)
df = pd.DataFrame()
chart = {}

try:
    # ==========================================
    # 2. æ ¸å¿ƒé€»è¾‘åŒº (Core Logic)
    # ==========================================
    
    # Step 1: è°ƒç”¨æ¥å£è·å–æ•°æ®
    print("Step 1: æ­£åœ¨è°ƒç”¨ ak.stock_news_em æ¥å£...")
    df_news = ak.stock_news_em(symbol=symbol)
    
    # å¼‚å¸¸æ ¡éªŒï¼šåˆ¤æ–­æ•°æ®æ˜¯å¦ä¸ºç©º
    if df_news is None or df_news.empty:
        raise ValueError(f"æ¥å£è¿”å›æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ä»£ç  {symbol} æ˜¯å¦æ­£ç¡®æˆ–æ¥å£æºæ˜¯å¦å¯ç”¨ã€‚")
    
    print(f"âœ… æ¥å£è°ƒç”¨æˆåŠŸï¼ŒåŸå§‹æ•°æ®è·å– {len(df_news)} æ¡")

    # Step 2: æ•°æ®æ¸…æ´—ä¸æ ‡å‡†åŒ–
    # æ˜ å°„åˆ—åä¸ºè‹±æ–‡ï¼Œæ–¹ä¾¿å·¥ç¨‹åŒ–å¤„ç†
    column_mapping = {
        'å…³é”®è¯': 'keyword',
        'æ–°é—»æ ‡é¢˜': 'title',
        'æ–°é—»å†…å®¹': 'content',
        'å‘å¸ƒæ—¶é—´': 'publish_time',
        'æ–‡ç« æ¥æº': 'source',
        'æ–°é—»é“¾æ¥': 'url'
    }
    
    # é‡å‘½ååˆ—
    # æ³¨æ„ï¼šä½¿ç”¨ errors='ignore' é˜²æ­¢æ¥å£å­—æ®µå˜åŠ¨å¯¼è‡´æŠ¥é”™
    df_news = df_news.rename(columns=column_mapping)
    
    # ä»…ä¿ç•™éœ€è¦çš„æ ‡å‡†åˆ— (é˜²æ­¢æ¥å£è¿”å›å¤šä½™å­—æ®µ)
    target_cols = ['keyword', 'title', 'source', 'publish_time', 'url']
    # ç¡®ä¿åˆ—å­˜åœ¨ï¼Œé˜²æ­¢KeyError
    available_cols = [col for col in target_cols if col in df_news.columns]
    df_clean = df_news[available_cols].copy()

    # Step 3: æ•°æ®æ ¼å¼å¤„ç†
    # ç¡®ä¿æ—¶é—´åˆ—æ˜¯å­—ç¬¦ä¸²æˆ–datetimeæ ¼å¼
    if 'publish_time' in df_clean.columns:
        df_clean['publish_time'] = df_clean['publish_time'].astype(str)
        
    # æŒ‰æ—¶é—´é™åºæ’åˆ— (æœ€æ–°çš„åœ¨æœ€å‰)
    df_clean = df_clean.sort_values(by='publish_time', ascending=False).reset_index(drop=True)

    # æˆªå–æŒ‡å®šæ•°é‡
    df_clean = df_clean.head(limit_count)

    print("âœ… æ•°æ®æ¸…æ´—å®Œæˆ")

    # ==========================================
    # 4. è¾“å‡ºæ„å»ºåŒº (Output Construction)
    # ==========================================
    
    # 4.1 å‡†å¤‡ Data
    df = df_clean
    
    # 4.2 å‡†å¤‡ Chart
    # è¯´æ˜ï¼šæ–°é—»ä¸»è¦ä¸ºæ–‡æœ¬æ•°æ®ï¼Œæš‚æ— æ ‡å‡†æŠ˜çº¿/æŸ±çŠ¶å›¾å±•ç¤ºéœ€æ±‚ã€‚
    # è¿™é‡Œè¿”å›ä¸€ä¸ªç©ºçš„é…ç½®å­—å…¸ï¼Œæˆ–è€…å¯ä»¥é…ç½®ä¸€ä¸ªç®€å•çš„è¯äº‘æ•°æ®(å¦‚æœæœ‰NLPåº“æ”¯æŒ)
    # æ­¤å¤„ä¿æŒä¸ºç©ºï¼Œç¬¦åˆçº¯æ•°æ®è„šæœ¬è§„èŒƒã€‚
    chart = {} 
    
    print(f"âœ… [Success] è„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼Œå…±è¾“å‡º {len(df)} æ¡æ–°é—»ã€‚")
    print(f"   æœ€æ–°ä¸€æ¡: [{df.iloc[0]['publish_time']}] {df.iloc[0]['title']}")

except Exception as e:
    # ==========================================
    # 3. å¼‚å¸¸å¤„ç†åŒº (Exception Handling)
    # ==========================================
    print(f"âŒ [Error] è„šæœ¬æ‰§è¡Œå‡ºé”™: {e}")
    # å…œåº•è¿”å›ç©º DataFrameï¼Œé˜²æ­¢ä¸‹æ¸¸ç¨‹åºå´©æºƒ
    df = pd.DataFrame()
    chart = {}
```

## æ‹‰å–æŒ‡å®šè‚¡ç¥¨æ–°é—»åˆ—è¡¨

- idï¼š`7`
- is_pinnedï¼š`0`
- updated_atï¼š`2026-01-05 02:40:39`
- last_run_statusï¼š`pending`

### script_content

```python
# Write your Python script here
# Define "df" (DataFrame) or "result" (List) for table
# Define "chart" (Dict) for visualization

import akshare as ak
import pandas as pd

try:
    stock_news = ak.stock_news_em(symbol="603986")
    print(f"è´µå·èŒ…å°æ–°é—»æ¡æ•°ï¼š{len(stock_news)}")

    # Columns are in Chinese: ['å…³é”®è¯', 'æ–°é—»æ ‡é¢˜', 'æ–°é—»å†…å®¹', 'å‘å¸ƒæ—¶é—´', 'æ–‡ç« æ¥æº', 'æ–°é—»é“¾æ¥']
    # Select relevant columns and rename them to match expected output
    df = stock_news[["æ–°é—»æ ‡é¢˜", "å‘å¸ƒæ—¶é—´", "æ–‡ç« æ¥æº","æ–°é—»é“¾æ¥"]].copy()
    df.columns = ["title", "pub_date", "src","url"]

    print(df.head())
    print("Hello Research Lab")
except Exception as e:
    print(f"Error fetching news: {e}")
```

## çƒ­é—¨æ¿å—ä¸»çº¿åˆ¤æ–­

- idï¼š`6`
- is_pinnedï¼š`0`
- updated_atï¼š`2026-01-04 08:04:25`
- last_run_statusï¼š`pending`

### script_content

```python
# å¯¼å…¥å¿…è¦ä¾èµ–åº“
import akshare as ak
import pandas as pd
from datetime import datetime, timedelta
from dateutil.parser import parse
import warnings
import pyperclip  # æ–°å¢ï¼šç”¨äºæ“ä½œå‰ªè´´æ¿
warnings.filterwarnings("ignore")

# ====================== ç¬¬ä¸€æ­¥ï¼šæ ¸å¿ƒå‚æ•°é›†ä¸­é…ç½®ï¼ˆæ›´æ˜“ç»´æŠ¤ï¼‰ ======================
print("=== è¿‘ä¸€ä¸ªæœˆæ¿å—èµ„é‡‘æµå‘ä¸»çº¿è¯†åˆ«è„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼Œå«å‰ªè´´æ¿åŠŸèƒ½ï¼‰ ===")
# æ ¸å¿ƒå‚æ•°ï¼ˆé›†ä¸­ç®¡ç†ï¼Œä¸€é”®è°ƒæ•´ï¼‰
CONFIG = {
    "TIME_RANGE": 30,                # ç»Ÿè®¡è¿‘30å¤©æ•°æ®
    "TOP_N": 5,                      # æ¯æ—¥èµ„é‡‘å‡€æµå…¥å‰5å
    "CONTINUOUS_DAYS": 3,            # è¿ç»­ä¸Šæ¦œé˜ˆå€¼
    "MIN_FREQ": 3,                   # æœ€ä½ä¸Šæ¦œé¢‘æ¬¡
    "AVG_RANK_TOLERANCE": 2,         # å¹³å‡æ’åå®½æ¾åº¦
    "MAX_OUTPUT_ROWS": 100,          # å‰ç«¯è¾“å‡ºæœ€å¤§è¡Œæ•°
    "VALID_FLOW_THRESHOLD": 0        # ä¸»åŠ›å‡€æµå…¥æœ€ä½é˜ˆå€¼ï¼ˆå‰”é™¤å‡€æµå‡ºï¼‰
}
# æ‰“å°é…ç½®ä¿¡æ¯
print(f"æ‰§è¡Œæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
for key, val in CONFIG.items():
    print(f"{key}ï¼š{val}")
print(f"æ•°æ®æ¥å£ï¼šak.stock_sector_fund_flow_hist() + ak.stock_board_industry_hist_em()")
print("=" * 60)

# ====================== ç¬¬äºŒæ­¥ï¼šä¼˜åŒ–äº¤æ˜“æ—¥è·å–ï¼ˆæ›´ç¨³å®šã€å®¹é”™æ€§æ›´å¼ºï¼‰ ======================
def get_trading_days_recent(days=30):
    """
    ä¼˜åŒ–ç‰ˆï¼šè·å–è¿‘Nä¸ªè‡ªç„¶æ—¥å†…çš„Aè‚¡äº¤æ˜“æ—¥
    äº®ç‚¹ï¼šåŒé‡å…œåº• + æ—¥æœŸæ ¼å¼ç»Ÿä¸€ + å¼‚å¸¸è¯¦ç»†æç¤º
    """
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)
    end_date_str = end_date.strftime("%Y%m%d")
    start_date_str = start_date.strftime("%Y%m%d")

    try:
        # ä¼˜å…ˆï¼šé€šè¿‡ä¸Šè¯æŒ‡æ•°è·å–äº¤æ˜“æ—¥ï¼ˆç¨³å®šå¯é ï¼‰
        trade_date_df = ak.index_zh_a_hist(
            symbol="000001",
            period="daily",
            start_date=start_date_str,
            end_date=end_date_str
        )
        trade_date_df = trade_date_df.rename(columns={"æ—¥æœŸ": "trade_date"})
        trade_date_df["trade_date"] = pd.to_datetime(trade_date_df["trade_date"])
        recent_trade_dates = trade_date_df["trade_date"].dt.date.tolist()

    except Exception as e1:
        try:
            # å…œåº•1ï¼šä½¿ç”¨Aè‚¡äº¤æ˜“æ—¥å†æ¥å£
            trade_date_df = ak.stock_trade_date_hist_sina()
            trade_date_df["trade_date"] = pd.to_datetime(trade_date_df["trade_date"])
            trade_date_df = trade_date_df[
                (trade_date_df["trade_date"] >= start_date) &
                (trade_date_df["trade_date"] <= end_date)
            ]
            recent_trade_dates = trade_date_df["trade_date"].dt.date.tolist()

        except Exception as e2:
            raise Exception(f"äº¤æ˜“æ—¥è·å–å¤±è´¥ï¼ˆåŒé‡å…œåº•å‡å¤±æ•ˆï¼‰ï¼š\n1. ä¸Šè¯æŒ‡æ•°æ¥å£é”™è¯¯ï¼š{e1}\n2. äº¤æ˜“æ—¥å†æ¥å£é”™è¯¯ï¼š{e2}")

    # å»é‡ + æ’åºï¼ˆé¿å…å¼‚å¸¸æ•°æ®ï¼‰
    recent_trade_dates = sorted(list(set(recent_trade_dates)))
    return recent_trade_dates

# è·å–äº¤æ˜“æ—¥åˆ—è¡¨å¹¶æ ¡éªŒ
recent_trade_dates = get_trading_days_recent(CONFIG["TIME_RANGE"])
print(f"è¿‘{CONFIG['TIME_RANGE']}å¤©å†…Aè‚¡äº¤æ˜“æ—¥æ•°é‡ï¼š{len(recent_trade_dates)}")
if len(recent_trade_dates) == 0:
    raise Exception(f"æœªè·å–åˆ°æœ‰æ•ˆäº¤æ˜“æ—¥ï¼ˆæ—¥æœŸèŒƒå›´ï¼š{start_date_str} - {end_date_str}ï¼‰")
# ä¼˜åŒ–æ—¥æœŸå±•ç¤ºï¼ˆé¦–å°¾å„5ä¸ªï¼Œæ›´ç›´è§‚ï¼‰
date_display = recent_trade_dates[:5] + ["..."] + recent_trade_dates[-5:] if len(recent_trade_dates) > 10 else recent_trade_dates
print(f"äº¤æ˜“æ—¥åˆ—è¡¨ï¼š{date_display}")
print("=" * 60)

# ====================== ç¬¬ä¸‰æ­¥ï¼šæ‰¹é‡æŠ“å–æ¿å—æ•°æ®ï¼ˆæ€§èƒ½ä¼˜åŒ– + è¿›åº¦å¯è§†åŒ–ï¼‰ ======================
def get_all_sector_list():
    """ä¼˜åŒ–ç‰ˆï¼šè·å–è¡Œä¸šæ¿å—åˆ—è¡¨ï¼Œå¸¦å®¹é”™"""
    try:
        sector_list_df = ak.stock_board_industry_name_em()
        sectors = sector_list_df['æ¿å—åç§°'].drop_duplicates().tolist()  # å»é‡
        invalid_sectors = ["", "æœªçŸ¥", "ç»¼åˆ"]  # è¿‡æ»¤æ— æ•ˆæ¿å—
        sectors = [s for s in sectors if s not in invalid_sectors]
        return sectors
    except Exception as e:
        raise Exception(f"è·å–æ¿å—åˆ—è¡¨å¤±è´¥ï¼š{e}")

def batch_get_sector_fund_flow(sectors, valid_dates):
    """
    ä¼˜åŒ–ç‰ˆï¼šæ‰¹é‡è·å–æ¿å—èµ„é‡‘æµ
    äº®ç‚¹ï¼šè¿›åº¦æç¤º + æ— æ•ˆæ•°æ®è¿‡æ»¤ + å†…å­˜ä¼˜åŒ–
    """
    all_flow_list = []
    valid_dates_set = set(valid_dates)
    total_sectors = len(sectors)

    print(f"å¼€å§‹æ‰¹é‡è·å– {total_sectors} ä¸ªæ¿å—èµ„é‡‘æµï¼ˆé¢„è®¡è€—æ—¶1-2åˆ†é’Ÿï¼‰...")
    for idx, sector in enumerate(sectors):
        # æ¯20ä¸ªæ¿å—æ‰“å°ä¸€æ¬¡è¿›åº¦ï¼ˆå‡å°‘æ—¥å¿—å†—ä½™ï¼‰
        if (idx + 1) % 20 == 0 or idx + 1 == total_sectors:
            print(f"  è¿›åº¦ï¼š{idx + 1}/{total_sectors}ï¼ˆ{((idx + 1)/total_sectors)*100:.1f}%ï¼‰")

        try:
            # è·å–å•ä¸ªæ¿å—èµ„é‡‘æµ
            flow_df = ak.stock_sector_fund_flow_hist(symbol=sector)
            if flow_df.empty:
                continue

            # æ—¥æœŸæ ¼å¼ç»Ÿä¸€ + èŒƒå›´ç­›é€‰
            flow_df['æ—¥æœŸ'] = pd.to_datetime(flow_df['æ—¥æœŸ']).dt.date
            flow_df = flow_df[flow_df['æ—¥æœŸ'].isin(valid_dates_set)].copy()

            # è¿‡æ»¤ä¸»åŠ›å‡€æµå…¥ä¸ºè´Ÿçš„è®°å½•ï¼ˆä»…ä¿ç•™èµ„é‡‘æµå…¥æ¿å—ï¼‰
            flow_df = flow_df[flow_df["ä¸»åŠ›å‡€æµå…¥-å‡€é¢"] >= CONFIG["VALID_FLOW_THRESHOLD"]].copy()

            if not flow_df.empty:
                flow_df['åç§°'] = sector
                all_flow_list.append(flow_df)
        except Exception as e:
            # é™é»˜è·³è¿‡å¤±è´¥æ¿å—ï¼Œä¸ä¸­æ–­æ•´ä½“æµç¨‹
            continue

    if not all_flow_list:
        raise Exception("æœªè·å–åˆ°ä»»ä½•æ¿å—æœ‰æ•ˆèµ„é‡‘æµæ•°æ®")

    # åˆå¹¶æ•°æ®å¹¶é‡Šæ”¾å†…å­˜
    all_flow_df = pd.concat(all_flow_list, ignore_index=True)
    del all_flow_list  # é‡Šæ”¾ä¸´æ—¶åˆ—è¡¨å†…å­˜
    return all_flow_df

# 1. è·å–æœ‰æ•ˆæ¿å—åˆ—è¡¨
sectors = get_all_sector_list()
print(f"è·å–åˆ°æœ‰æ•ˆè¡Œä¸šæ¿å—æ•°é‡ï¼š{len(sectors)}")

# 2. æ‰¹é‡è·å–èµ„é‡‘æµæ•°æ®
all_flow_df = batch_get_sector_fund_flow(sectors, recent_trade_dates)
print(f"èµ„é‡‘æµæ•°æ®è·å–å®Œæˆï¼šå…± {len(all_flow_df)} æ¡æœ‰æ•ˆè®°å½•")
print("=" * 60)

# ====================== ç¬¬å››æ­¥ï¼šæŒ‰æ—¥ç­›é€‰Top Nï¼ˆé€»è¾‘ä¼˜åŒ– + å­—æ®µç»Ÿä¸€ï¼‰ ======================
def get_daily_top_n_sectors(all_flow_df, trade_dates, top_n):
    """
    ä¼˜åŒ–ç‰ˆï¼šæŒ‰æ—¥ç­›é€‰Top Næ¿å—
    äº®ç‚¹ï¼šæ—¥æœŸæ ¼å¼å…¼å®¹ + å­—æ®µæ ‡å‡†åŒ– + ç©ºæ•°æ®è·³è¿‡
    """
    daily_top_n_sectors = []
    trade_dates_str_map = {d: d.strftime("%Y%m%d") for d in trade_dates}  # æ—¥æœŸæ˜ å°„è¡¨

    for trade_date in trade_dates:
        # ç­›é€‰å½“æ—¥æ•°æ®
        daily_flow_df = all_flow_df[all_flow_df['æ—¥æœŸ'] == trade_date].copy()
        if daily_flow_df.empty:
            continue

        # æŒ‰ä¸»åŠ›å‡€æµå…¥å€’åºæ’åºï¼Œå–Top N
        daily_flow_df = daily_flow_df.sort_values("ä¸»åŠ›å‡€æµå…¥-å‡€é¢", ascending=False)
        top_n_df = daily_flow_df.head(top_n).copy()

        # æ·»åŠ è¾…åŠ©å­—æ®µ + å­—æ®µæ ‡å‡†åŒ–
        top_n_df['äº¤æ˜“æ—¥'] = trade_dates_str_map[trade_date]
        top_n_df['å½“æ—¥æ’å'] = range(1, len(top_n_df) + 1)
        top_n_df = top_n_df.rename(columns={
            "ä¸»åŠ›å‡€æµå…¥-å‡€é¢": "ä»Šæ—¥ä¸»åŠ›å‡€æµå…¥-å‡€é¢",
            "ä¸»åŠ›å‡€æµå…¥-å‡€å æ¯”": "ä»Šæ—¥ä¸»åŠ›å‡€æµå…¥-å‡€å æ¯”"
        })

        daily_top_n_sectors.append(top_n_df)

    if not daily_top_n_sectors:
        raise Exception("æœªèƒ½ç­›é€‰å‡ºæ¯æ—¥Top Næ¿å—")

    return pd.concat(daily_top_n_sectors, ignore_index=True)

# è·å–æ¯æ—¥Top Næ¿å—
all_top5_basic = get_daily_top_n_sectors(all_flow_df, recent_trade_dates, CONFIG["TOP_N"])

# ====================== ç¬¬äº”æ­¥ï¼šè¡¥å……æ¶¨è·Œå¹…æ•°æ®ï¼ˆä¼˜åŒ–å…œåº• + é«˜æ•ˆåˆå¹¶ï¼‰ ======================
def supplement_sector_price_data(involved_sectors, start_date, end_date):
    """
    ä¼˜åŒ–ç‰ˆï¼šè¡¥å……æ¿å—æ¶¨è·Œå¹…æ•°æ®
    äº®ç‚¹ï¼šä»…å¤„ç†ä¸Šæ¦œæ¿å— + ç¼ºå¤±å€¼å…œåº• + é«˜æ•ˆåˆå¹¶
    """
    price_data_list = []
    start_date_str = start_date.strftime("%Y%m%d")
    end_date_str = end_date.strftime("%Y%m%d")

    print(f"è¡¥å…… {len(involved_sectors)} ä¸ªä¸Šæ¦œæ¿å—æ¶¨è·Œå¹…æ•°æ®...")
    for sector in involved_sectors:
        try:
            price_df = ak.stock_board_industry_hist_em(
                symbol=sector,
                start_date=start_date_str,
                end_date=end_date_str
            )
            if price_df.empty:
                continue
            price_df['åç§°'] = sector
            price_df['æ—¥æœŸ'] = pd.to_datetime(price_df['æ—¥æœŸ']).dt.date
            price_data_list.append(price_df[['æ—¥æœŸ', 'åç§°', 'æ¶¨è·Œå¹…']])
        except Exception as e:
            continue

    # åˆå¹¶æ¶¨è·Œå¹…æ•°æ®
    if price_data_list:
        price_df = pd.concat(price_data_list, ignore_index=True)
        return price_df
    else:
        # å…œåº•ï¼šè¿”å›ç©ºDataFrameï¼Œåç»­å¡«å……é»˜è®¤å€¼
        return pd.DataFrame(columns=['æ—¥æœŸ', 'åç§°', 'æ¶¨è·Œå¹…'])

# 1. è·å–ä¸Šæ¦œæ¿å—åˆ—è¡¨
involved_sectors = all_top5_basic['åç§°'].unique()
# 2. è¡¥å……æ¶¨è·Œå¹…æ•°æ®
price_df = supplement_sector_price_data(
    involved_sectors,
    datetime.combine(recent_trade_dates[0], datetime.min.time()),
    datetime.combine(recent_trade_dates[-1], datetime.min.time())
)
# 3. é«˜æ•ˆåˆå¹¶æ•°æ®ï¼ˆæ¸…ç†å†—ä½™å­—æ®µï¼‰
all_top5_basic['temp_date'] = pd.to_datetime(all_top5_basic['äº¤æ˜“æ—¥']).dt.date
if not price_df.empty:
    all_top5_df = pd.merge(
        all_top5_basic,
        price_df,
        left_on=['åç§°', 'temp_date'],
        right_on=['åç§°', 'æ—¥æœŸ'],
        how='left'
    )
    all_top5_df['ä»Šæ—¥æ¶¨è·Œå¹…'] = all_top5_df['æ¶¨è·Œå¹…'].fillna(0.0)  # ç¼ºå¤±å€¼å¡«å……0
else:
    all_top5_df = all_top5_basic.copy()
    all_top5_df['ä»Šæ—¥æ¶¨è·Œå¹…'] = 0.0

# æ¸…ç†å†—ä½™å­—æ®µ + ä¿ç•™æ ¸å¿ƒåˆ—
final_cols = ["äº¤æ˜“æ—¥", "å½“æ—¥æ’å", "åç§°", "ä»Šæ—¥æ¶¨è·Œå¹…", "ä»Šæ—¥ä¸»åŠ›å‡€æµå…¥-å‡€é¢", "ä»Šæ—¥ä¸»åŠ›å‡€æµå…¥-å‡€å æ¯”"]
all_top5_df = all_top5_df[final_cols].copy()
del all_top5_basic  # é‡Šæ”¾å†…å­˜

# æ‰“å°æ•°æ®æ¦‚è§ˆ
print(f"ç´¯è®¡æŠ“å–Top5æ¿å—æ•°æ®ï¼š{len(all_top5_df)} æ¡")
print(f"æ¶‰åŠä¸Šæ¦œæ¿å—æ•°é‡ï¼š{all_top5_df['åç§°'].nunique()}")
print("=" * 60)

# ====================== ç¬¬å…­æ­¥ï¼šä¸»çº¿æ¿å—è¯†åˆ«ï¼ˆæŒ‡æ ‡ä¸°å¯Œ + ç­›é€‰çµæ´»ï¼‰ ======================
def calc_max_continuous_days_optimized(sector_name, all_df, trade_dates):
    """
    ä¼˜åŒ–ç‰ˆï¼šè®¡ç®—æœ€å¤§è¿ç»­ä¸Šæ¦œå¤©æ•°
    äº®ç‚¹ï¼šæ—¥æœŸæ˜ å°„å®¹é”™ + å¤„ç†éè¿ç»­äº¤æ˜“æ—¥ï¼ˆå‘¨äº”â†’å‘¨ä¸€ï¼‰
    """
    # è·å–æ¿å—ä¸Šæ¦œäº¤æ˜“æ—¥ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰
    sector_trade_str = sorted(all_df[all_df["åç§°"] == sector_name]["äº¤æ˜“æ—¥"].unique())
    if len(sector_trade_str) < 2:
        return len(sector_trade_str)

    # äº¤æ˜“æ—¥æ˜ å°„ï¼ˆdateå¯¹è±¡ â†’ å­—ç¬¦ä¸² â†’ ç´¢å¼•ï¼‰
    trade_date_str = [d.strftime("%Y%m%d") for d in trade_dates]
    trade_date_index = {date: idx for idx, date in enumerate(trade_date_str)}

    # è®¡ç®—è¿ç»­å¤©æ•°ï¼ˆæ”¯æŒå‘¨äº”â†’å‘¨ä¸€çš„è¿ç»­åˆ¤å®šï¼‰
    max_continuous = 1
    current_continuous = 1
    for i in range(1, len(sector_trade_str)):
        prev_str = sector_trade_str[i-1]
        curr_str = sector_trade_str[i]

        # å®¹é”™ï¼šè·³è¿‡ä¸åœ¨äº¤æ˜“æ—¥åˆ—è¡¨çš„æ—¥æœŸ
        if prev_str not in trade_date_index or curr_str not in trade_date_index:
            current_continuous = 1
            continue

        # è®¡ç®—ç´¢å¼•å·®
        idx_diff = trade_date_index[curr_str] - trade_date_index[prev_str]
        # è¿ç»­åˆ¤å®šï¼šç´¢å¼•å·®1ï¼ˆå·¥ä½œæ—¥è¿ç»­ï¼‰æˆ–3ï¼ˆå‘¨äº”â†’å‘¨ä¸€ï¼‰
        if idx_diff in [1, 3]:
            current_continuous += 1
            max_continuous = max(max_continuous, current_continuous)
        else:
            current_continuous = 1

    return max_continuous

# ç»´åº¦1ï¼šå¤šæŒ‡æ ‡ç»Ÿè®¡ï¼ˆæ–°å¢ä¸»åŠ›å‡€æµå…¥å‡å€¼ã€æœ€å¤§å•æ—¥å‡€æµå…¥ï¼‰
sector_freq_df = all_top5_df.groupby("åç§°").agg(
    ä¸Šæ¦œé¢‘æ¬¡=("äº¤æ˜“æ—¥", "nunique"),
    ç´¯è®¡ä¸»åŠ›å‡€æµå…¥=("ä»Šæ—¥ä¸»åŠ›å‡€æµå…¥-å‡€é¢", "sum"),
    å¹³å‡ä¸»åŠ›å‡€æµå…¥=("ä»Šæ—¥ä¸»åŠ›å‡€æµå…¥-å‡€é¢", "mean"),  # æ–°å¢ï¼šè¯„ä¼°å•æ—¥èµ„é‡‘å¼ºåº¦
    æœ€å¤§å•æ—¥å‡€æµå…¥=("ä»Šæ—¥ä¸»åŠ›å‡€æµå…¥-å‡€é¢", "max"),  # æ–°å¢ï¼šè¯„ä¼°æç«¯èµ„é‡‘æµå…¥
    å¹³å‡æ¶¨è·Œå¹…=("ä»Šæ—¥æ¶¨è·Œå¹…", "mean"),
    å¹³å‡æ’å=("å½“æ—¥æ’å", "mean")
).reset_index()

# ç»´åº¦2ï¼šä¼˜åŒ–è¿ç»­ä¸Šæ¦œå¤©æ•°è®¡ç®—
sector_freq_df["æœ€å¤§è¿ç»­ä¸Šæ¦œå¤©æ•°"] = sector_freq_df["åç§°"].apply(
    lambda x: calc_max_continuous_days_optimized(x, all_top5_df, recent_trade_dates)
)

# ç»´åº¦3ï¼šçµæ´»ç­›é€‰ä¸»çº¿æ¿å—ï¼ˆæ–°å¢æ­£å‘æ¶¨è·Œå¹…ç­›é€‰ï¼‰
main_sector_condition = (
    (sector_freq_df["ä¸Šæ¦œé¢‘æ¬¡"] >= CONFIG["MIN_FREQ"]) &
    (sector_freq_df["æœ€å¤§è¿ç»­ä¸Šæ¦œå¤©æ•°"] >= CONFIG["CONTINUOUS_DAYS"]) &
    (sector_freq_df["å¹³å‡æ’å"] <= CONFIG["TOP_N"] + CONFIG["AVG_RANK_TOLERANCE"]) &
    (sector_freq_df["å¹³å‡æ¶¨è·Œå¹…"] > 0)  # æ–°å¢ï¼šä»…ä¿ç•™å¹³å‡ä¸Šæ¶¨çš„æ¿å—ï¼ˆå‰”é™¤èµ„é‡‘æµå…¥ä½†ä¸‹è·Œçš„æ— æ•ˆæ¿å—ï¼‰
)
main_sector_df = sector_freq_df[main_sector_condition].copy()

# æŒ‰ç»¼åˆå¼ºåº¦æ’åºï¼ˆç´¯è®¡èµ„é‡‘ä¼˜å…ˆï¼Œå…¶æ¬¡å¹³å‡æ¶¨è·Œå¹…ï¼‰
main_sector_df = main_sector_df.sort_values(
    by=["ç´¯è®¡ä¸»åŠ›å‡€æµå…¥", "å¹³å‡æ¶¨è·Œå¹…"],
    ascending=False
).reset_index(drop=True)

# ====================== ç¬¬ä¸ƒæ­¥ï¼šæ„å»ºMDæ ¼å¼å†…å®¹ + å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼ˆæ–°å¢æ ¸å¿ƒåŠŸèƒ½ï¼‰ ======================
def build_md_content(main_sector_df, sector_freq_df):
    """
    æ„å»ºè§„èŒƒçš„Markdownæ ¼å¼å†…å®¹
    åŒ…å«ï¼šæ ¸å¿ƒç»Ÿè®¡ã€ä¸»çº¿æ¿å—ã€ä¸Šæ¦œé¢‘æ¬¡TOP5ã€ç´¯è®¡èµ„é‡‘TOP5
    """
    md_content = f"# è¿‘{CONFIG['TIME_RANGE']}å¤©æ¿å—èµ„é‡‘æµå‘ç»Ÿè®¡ç»“æœ\n\n"
    md_content += f"**ç»Ÿè®¡æ—¶é—´**ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    md_content += f"**é…ç½®å‚æ•°**ï¼šTOP_N={CONFIG['TOP_N']} | è¿ç»­ä¸Šæ¦œé˜ˆå€¼={CONFIG['CONTINUOUS_DAYS']} | æœ€ä½ä¸Šæ¦œé¢‘æ¬¡={CONFIG['MIN_FREQ']}\n\n"

    # 1. ä¸»çº¿æ¿å—ï¼ˆMDè¡¨æ ¼ï¼‰
    md_content += "## ä¸€ã€ä¸»çº¿æ¿å—åˆ—è¡¨\n\n"
    if len(main_sector_df) > 0:
        # MDè¡¨æ ¼å¤´
        md_content += "| æ’å | æ¿å—åç§° | ä¸Šæ¦œé¢‘æ¬¡ | è¿ç»­ä¸Šæ¦œå¤©æ•° | ç´¯è®¡ä¸»åŠ›å‡€æµå…¥ï¼ˆå…ƒï¼‰ | å¹³å‡æ¶¨è·Œå¹…ï¼ˆ%ï¼‰ | å¹³å‡æ’å |\n"
        md_content += "| ---- | -------- | -------- | ------------ | -------------------- | --------------- | -------- |\n"
        # å¡«å……è¡¨æ ¼å†…å®¹
        for idx, row in main_sector_df.iterrows():
            md_content += f"| {idx+1} | {row['åç§°']} | {row['ä¸Šæ¦œé¢‘æ¬¡']} | {row['æœ€å¤§è¿ç»­ä¸Šæ¦œå¤©æ•°']} | {row['ç´¯è®¡ä¸»åŠ›å‡€æµå…¥']:.0f} | {row['å¹³å‡æ¶¨è·Œå¹…']:.2f} | {row['å¹³å‡æ’å']:.1f} |\n"
    else:
        md_content += "æš‚æ— æ»¡è¶³æ¡ä»¶çš„ä¸»çº¿æ¿å—ï¼ˆå¯è°ƒæ•´CONFIGå‚æ•°é™ä½ç­›é€‰æ ‡å‡†ï¼‰\n\n"

    # 2. ä¸Šæ¦œé¢‘æ¬¡TOP5ï¼ˆMDåˆ—è¡¨/è¡¨æ ¼ï¼‰
    md_content += "## äºŒã€ä¸Šæ¦œé¢‘æ¬¡TOP5æ¿å—\n\n"
    top5_freq = sector_freq_df.sort_values("ä¸Šæ¦œé¢‘æ¬¡", ascending=False).head(5)
    # MDè¡¨æ ¼
    md_content += "| æ’å | æ¿å—åç§° | ä¸Šæ¦œé¢‘æ¬¡ | è¿ç»­ä¸Šæ¦œå¤©æ•° | å¹³å‡æ’å |\n"
    md_content += "| ---- | -------- | -------- | ------------ | -------- |\n"
    for idx, (_, row) in enumerate(top5_freq.iterrows(), 1):
        md_content += f"| {idx} | {row['åç§°']} | {row['ä¸Šæ¦œé¢‘æ¬¡']} | {row['æœ€å¤§è¿ç»­ä¸Šæ¦œå¤©æ•°']} | {row['å¹³å‡æ’å']:.1f} |\n"
    md_content += "\n"

    # 3. ç´¯è®¡ä¸»åŠ›å‡€æµå…¥TOP5ï¼ˆMDè¡¨æ ¼ï¼‰
    md_content += "## ä¸‰ã€ç´¯è®¡ä¸»åŠ›å‡€æµå…¥TOP5æ¿å—\n\n"
    top5_amount = sector_freq_df.sort_values("ç´¯è®¡ä¸»åŠ›å‡€æµå…¥", ascending=False).head(5)
    md_content += "| æ’å | æ¿å—åç§° | ç´¯è®¡ä¸»åŠ›å‡€æµå…¥ï¼ˆå…ƒï¼‰ | å¹³å‡ä¸»åŠ›å‡€æµå…¥ï¼ˆå…ƒï¼‰ | ä¸Šæ¦œé¢‘æ¬¡ |\n"
    md_content += "| ---- | -------- | -------------------- | -------------------- | -------- |\n"
    for idx, (_, row) in enumerate(top5_amount.iterrows(), 1):
        md_content += f"| {idx} | {row['åç§°']} | {row['ç´¯è®¡ä¸»åŠ›å‡€æµå…¥']:.0f} | {row['å¹³å‡ä¸»åŠ›å‡€æµå…¥']:.0f} | {row['ä¸Šæ¦œé¢‘æ¬¡']} |\n"
    md_content += "\n"

    # 4. å…¨æ¿å—ç»Ÿè®¡ï¼ˆå¯é€‰ï¼Œç²¾ç®€ç‰ˆï¼‰
    md_content += "## å››ã€å…¨æ¿å—æ ¸å¿ƒç»Ÿè®¡ï¼ˆç²¾ç®€ï¼‰\n\n"
    md_content += f"å…±æ¶‰åŠ {sector_freq_df['åç§°'].nunique()} ä¸ªæ¿å—ï¼Œä»¥ä¸‹ä¸ºå…³é”®æŒ‡æ ‡æ±‡æ€»ï¼š\n\n"
    md_content += "| æ¿å—åç§° | ä¸Šæ¦œé¢‘æ¬¡ | ç´¯è®¡ä¸»åŠ›å‡€æµå…¥ï¼ˆå…ƒï¼‰ | æœ€å¤§è¿ç»­ä¸Šæ¦œå¤©æ•° | å¹³å‡æ¶¨è·Œå¹…ï¼ˆ%ï¼‰ |\n"
    md_content += "| -------- | -------- | -------------------- | ------------ | --------------- |\n"
    # ä»…å–å‰20ä¸ªï¼ˆé¿å…å†…å®¹è¿‡é•¿ï¼‰
    for _, row in sector_freq_df.head(20).iterrows():
        md_content += f"| {row['åç§°']} | {row['ä¸Šæ¦œé¢‘æ¬¡']} | {row['ç´¯è®¡ä¸»åŠ›å‡€æµå…¥']:.0f} | {row['æœ€å¤§è¿ç»­ä¸Šæ¦œå¤©æ•°']} | {row['å¹³å‡æ¶¨è·Œå¹…']:.2f} |\n"

    return md_content

def copy_md_to_clipboard(md_content):
    """
    å¤åˆ¶MDå†…å®¹åˆ°å‰ªè´´æ¿ï¼Œå¸¦å¼‚å¸¸å¤„ç†
    """
    try:
        pyperclip.copy(md_content)
        print("âœ… MDæ ¼å¼ç»Ÿè®¡ç»“æœå·²æˆåŠŸå¤åˆ¶åˆ°å‰ªè´´æ¿ï¼")
        print("ğŸ’¡ å¯ç›´æ¥ç²˜è´´åˆ°Markdownç¼–è¾‘å™¨ï¼ˆå¦‚Notionã€Typoraã€ç®€ä¹¦ç­‰ï¼‰ä½¿ç”¨")
    except Exception as e:
        print(f"âŒ å‰ªè´´æ¿å¤åˆ¶å¤±è´¥ï¼š{e}")
        print("ğŸ“Œ è§£å†³æ–¹æ¡ˆï¼š1. å®‰è£…pyperclipï¼ˆpip install pyperclipï¼‰ï¼›2. æ£€æŸ¥ç³»ç»Ÿå‰ªè´´æ¿æ˜¯å¦è¢«å ç”¨")

# æ„å»ºMDå†…å®¹å¹¶å¤åˆ¶
md_content = build_md_content(main_sector_df, sector_freq_df)
copy_md_to_clipboard(md_content)

# ====================== ç¬¬å…«æ­¥ï¼šè¾“å‡ºç»“æœï¼ˆä¼˜åŒ–æ—¥å¿— + å›¾è¡¨å…¼å®¹æ€§ï¼‰ ======================
# 1. ä¼˜åŒ–Logè¾“å‡ºï¼ˆæ›´æ¸…æ™°çš„æ ¸å¿ƒç»“è®ºï¼‰
print("=" * 60)
print("=== è¿‘ä¸€ä¸ªæœˆæ¿å—èµ„é‡‘æµå‘æ ¸å¿ƒç»Ÿè®¡ ===")
# ä¸Šæ¦œé¢‘æ¬¡TOP5
print(f"1. ä¸Šæ¦œé¢‘æ¬¡TOP5æ¿å—ï¼š")
top5_freq = sector_freq_df.sort_values("ä¸Šæ¦œé¢‘æ¬¡", ascending=False).head(5)
for _, row in top5_freq.iterrows():
    print(f"   {row['åç§°']}ï¼š{row['ä¸Šæ¦œé¢‘æ¬¡']}æ¬¡ | å¹³å‡æ’å{row['å¹³å‡æ’å']:.1f} | è¿ç»­{row['æœ€å¤§è¿ç»­ä¸Šæ¦œå¤©æ•°']}å¤©")
# ç´¯è®¡èµ„é‡‘TOP5
print(f"2. ç´¯è®¡ä¸»åŠ›å‡€æµå…¥TOP5æ¿å—ï¼š")
top5_amount = sector_freq_df.sort_values("ç´¯è®¡ä¸»åŠ›å‡€æµå…¥", ascending=False).head(5)
for _, row in top5_amount.iterrows():
    print(f"   {row['åç§°']}ï¼š{row['ç´¯è®¡ä¸»åŠ›å‡€æµå…¥']:.0f}å…ƒ | å¹³å‡å‡€æµå…¥{row['å¹³å‡ä¸»åŠ›å‡€æµå…¥']:.0f}å…ƒ")
# ä¸»çº¿æ¿å—ç»“æœ
print(f"3. è¯†åˆ«å‡ºä¸»çº¿æ¿å—æ•°é‡ï¼š{len(main_sector_df)}ä¸ª")
if len(main_sector_df) > 0:
    print(f"4. ä¸»çº¿æ¿å—åˆ—è¡¨ï¼ˆæŒ‰èµ„é‡‘å¼ºåº¦æ’åºï¼‰ï¼š")
    for idx, row in main_sector_df.iterrows():
        print(f"   ã€{idx+1}ã€‘{row['åç§°']}ï¼š{row['ä¸Šæ¦œé¢‘æ¬¡']}æ¬¡ä¸Šæ¦œ | {row['æœ€å¤§è¿ç»­ä¸Šæ¦œå¤©æ•°']}å¤©è¿ç»­ | ç´¯è®¡{row['ç´¯è®¡ä¸»åŠ›å‡€æµå…¥']:.0f}å…ƒ | å¹³å‡æ¶¨å¹…{row['å¹³å‡æ¶¨è·Œå¹…']:.2f}%")
else:
    print(f"4. æš‚æ— æ»¡è¶³æ¡ä»¶çš„ä¸»çº¿æ¿å—ï¼ˆå»ºè®®è°ƒæ•´é…ç½®ï¼šCONTINUOUS_DAYS={CONFIG['CONTINUOUS_DAYS']} â†’ 2 æˆ– MIN_FREQ={CONFIG['MIN_FREQ']} â†’ 2ï¼‰")
print("=" * 60)

# 2. Dataè¾“å‡ºï¼ˆä¼˜åŒ–åˆå¹¶é€»è¾‘ + ä¸»é”®å”¯ä¸€æ€§ï¼‰
sector_freq_df["æ•°æ®ç±»å‹"] = "å…¨æ¿å—ç»Ÿè®¡"
main_sector_df["æ•°æ®ç±»å‹"] = "ä¸»çº¿æ¿å—"
# ä¼˜å…ˆå±•ç¤ºä¸»çº¿æ¿å—ï¼Œå†å±•ç¤ºå…¨æ¿å—ç»Ÿè®¡
final_df = pd.concat([main_sector_df, sector_freq_df], ignore_index=True)
# å»é‡ï¼ˆé¿å…åŒä¸€æ¿å—é‡å¤å±•ç¤ºï¼‰
final_df = final_df.drop_duplicates(subset=["åç§°"], keep="first")
# æ·»åŠ å”¯ä¸€ä¸»é”® + æ§åˆ¶è¾“å‡ºè¡Œæ•°
final_df["æ¿å—ç¼–ç "] = [f"SECTOR_{i+1:03d}" for i in range(len(final_df))]  # ä¸‰ä½æ•°ç¼–ç ï¼Œæ›´è§„èŒƒ
final_df = final_df.head(CONFIG["MAX_OUTPUT_ROWS"]).reset_index(drop=True)

# 3. Charté…ç½®ï¼ˆä¼˜åŒ–æ•°æ®é€‚é… + é‡çº§æç¤ºï¼‰
chart = {}
# é€‰æ‹©å¯è§†åŒ–æ•°æ®ï¼ˆä¸»çº¿æ¿å—ä¼˜å…ˆï¼Œå¦åˆ™å–ä¸Šæ¦œé¢‘æ¬¡TOP10ï¼‰
plot_df = main_sector_df if len(main_sector_df) > 0 else sector_freq_df.sort_values("ä¸Šæ¦œé¢‘æ¬¡", ascending=False).head(10)

if len(plot_df) > 0:
    chart = {
        "xKey": "åç§°",
        "series": [
            {
                "key": "ä¸Šæ¦œé¢‘æ¬¡",
                "type": "bar",
                "color": "#1890ff",
                "name": "ä¸Šæ¦œé¢‘æ¬¡"
            },
            {
                "key": "ç´¯è®¡ä¸»åŠ›å‡€æµå…¥",
                "type": "line",
                "color": "#f5222d",
                "name": "ç´¯è®¡ä¸»åŠ›å‡€æµå…¥ï¼ˆå…ƒï¼‰"
            }
        ]
    }
    # æ‰“å°å›¾è¡¨æç¤ºï¼ˆè§£å†³é‡çº§å·®å¼‚é—®é¢˜ï¼‰
    if len(main_sector_df) > 0:
        print(f"å›¾è¡¨é…ç½®ç”ŸæˆæˆåŠŸï¼šå±•ç¤º {len(plot_df)} ä¸ªä¸»çº¿æ¿å—ï¼ˆæ³¨ï¼šæŠ˜çº¿å›¾ä¸ºç´¯è®¡èµ„é‡‘ï¼ŒæŸ±çŠ¶å›¾ä¸ºä¸Šæ¦œé¢‘æ¬¡ï¼Œé‡çº§ä¸åŒä»…ä¾›è¶‹åŠ¿å‚è€ƒï¼‰")
    else:
        print(f"å›¾è¡¨é…ç½®ç”ŸæˆæˆåŠŸï¼šå±•ç¤ºä¸Šæ¦œé¢‘æ¬¡TOP10æ¿å—ï¼ˆæ— å¼ºä¸»çº¿æ¿å—ï¼Œä»…ä¾›å‚è€ƒï¼‰")
else:
    print(f"æ— æœ‰æ•ˆæ¿å—æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨")

print("=== è„šæœ¬æ‰§è¡Œå®Œæˆ ===")

# æœ€ç»ˆè¾“å‡ºDataï¼ˆç¬¦åˆé¡¹ç›®è§„èŒƒï¼‰
df = final_df
```

## è·å–å…¨å¸‚åœº pe

- idï¼š`1`
- is_pinnedï¼š`0`
- updated_atï¼š`2025-12-26 08:39:33`
- last_run_statusï¼š`pending`

### script_content

```python
df= ak.stock_zh_a_spot_em()
df=df[[
"ä»£ç ","åç§°","å¸‚ç›ˆç‡-åŠ¨æ€"
]]
df=df.sort_values(by="å¸‚ç›ˆç‡-åŠ¨æ€",ascending=False)
```

## ä¹–ç¦»ç‡è®¡ç®—

- idï¼š`5`
- is_pinnedï¼š`0`
- updated_atï¼š`None`
- last_run_statusï¼š`pending`

### script_content

```python
import akshare as ak
import pandas as pd

# 1. è·å–ä¸ªè‚¡å†å²è¡Œæƒ… (å‰å¤æƒ)
stock_code = "300900"
df = ak.stock_zh_a_hist(symbol=stock_code, period="daily", adjust="qfq")

# 2. è®¡ç®—ç§»åŠ¨å¹³å‡çº¿ (MA)
df['MA5'] = df['æ”¶ç›˜'].rolling(window=5).mean()
df['MA10'] = df['æ”¶ç›˜'].rolling(window=10).mean()
df['MA20'] = df['æ”¶ç›˜'].rolling(window=20).mean()

# 3. è®¡ç®—ä¹–ç¦»ç‡ (BIAS)
df['BIAS5'] = (df['æ”¶ç›˜'] - df['MA5']) / df['MA5'] * 100
df['BIAS10'] = (df['æ”¶ç›˜'] - df['MA10']) / df['MA10'] * 100
df['BIAS20'] = (df['æ”¶ç›˜'] - df['MA20']) / df['MA20'] * 100

# 4. æå– 2025-12-31 çš„æ•°æ®
target_date = "2025-12-31"
row = df[df['æ—¥æœŸ'] == target_date]

if not row.empty:
    print(f"=== å¹¿è”èˆªç©º ({stock_code}) {target_date} ä¹–ç¦»ç‡ ===")
    print(f"æ”¶ç›˜ä»·: {row['æ”¶ç›˜'].values[0]}")
    print(f"BIAS-5  (5æ—¥ä¹–ç¦»):  {row['BIAS5'].values[0]:.2f}%")
    print(f"BIAS-10 (10æ—¥ä¹–ç¦»): {row['BIAS10'].values[0]:.2f}%")
    print(f"BIAS-20 (20æ—¥ä¹–ç¦»): {row['BIAS20'].values[0]:.2f}%")
else:
    print("æœªæ‰¾åˆ°å¯¹åº”æ—¥æœŸçš„äº¤æ˜“æ•°æ®")
```

## å…¬å¸å…¬å‘Šè·å–

- idï¼š`8`
- is_pinnedï¼š`0`
- updated_atï¼š`None`
- last_run_statusï¼š`pending`

### script_content

```python
import akshare as ak
# è·å–å¹³å®‰é“¶è¡Œï¼ˆ000001ï¼‰2023.06.19-2023.12.20æœŸé—´çš„â€œå…¬å¸æ²»ç†â€ç±»å…¬å‘Š
df = ak.stock_zh_a_disclosure_report_cninfo(
    symbol="000001", 
    market="æ²ªæ·±äº¬", 
    category="å…¬å¸æ²»ç†", 
    start_date="20230619", 
    end_date="20231220"
)
print(df)
```

## streamlit ä¾‹å­

- idï¼š`11`
- is_pinnedï¼š`0`
- updated_atï¼š`None`
- last_run_statusï¼š`pending`

### script_content

```python
import streamlit as st
import pandas as pd
import numpy as np
import time

# è®¾ç½®é¡µé¢é…ç½®ï¼ˆå¿…é¡»æ”¾åœ¨æ‰€æœ‰streamlitå‘½ä»¤æœ€å¼€å¤´ï¼‰
st.set_page_config(
    page_title="Streamlit åŸºç¡€ç¤ºä¾‹",
    page_icon="âœ¨",
    layout="wide"  # å®½å±å¸ƒå±€
)

# 1. æ ‡é¢˜å’Œæ–‡æœ¬å±•ç¤º
st.title("Streamlit å¿«é€Ÿå…¥é—¨ç¤ºä¾‹")
st.subheader("è¿™æ˜¯äºŒçº§æ ‡é¢˜")
st.write("Streamlitæ ¸å¿ƒä¼˜åŠ¿ï¼šæ— éœ€å‰ç«¯çŸ¥è¯†ï¼Œçº¯Pythonå¿«é€Ÿæ„å»ºæ•°æ®åº”ç”¨ï¼")
st.markdown("### æ”¯æŒMarkdownè¯­æ³•\n- åˆ—è¡¨é¡¹1\n- åˆ—è¡¨é¡¹2\n**ç²—ä½“æ–‡æœ¬** | *æ–œä½“æ–‡æœ¬*")

# 2. äº¤äº’å¼è¾“å…¥ç»„ä»¶
name = st.text_input("è¯·è¾“å…¥ä½ çš„åå­—", placeholder="ä¾‹å¦‚ï¼šå¼ ä¸‰")
age = st.slider("è¯·é€‰æ‹©ä½ çš„å¹´é¾„", min_value=0, max_value=100, value=25)
favorite_color = st.selectbox("é€‰æ‹©ä½ æœ€å–œæ¬¢çš„é¢œè‰²", ["çº¢è‰²", "è“è‰²", "ç»¿è‰²", "é»„è‰²"])

# æŒ‰é’®è§¦å‘é€»è¾‘
if st.button("æäº¤ä¿¡æ¯"):
    st.success(f"ä½ å¥½ {name}ï¼ä½ çš„å¹´é¾„æ˜¯ {age} å²ï¼Œæœ€å–œæ¬¢çš„é¢œè‰²æ˜¯ {favorite_color} ğŸ¨")

# 3. æ•°æ®å±•ç¤º
st.subheader("ç”Ÿæˆéšæœºæ•°æ®å¹¶å±•ç¤º")
# ç”Ÿæˆ50è¡Œ3åˆ—çš„éšæœºæ•°æ®
data = pd.DataFrame(
    np.random.randn(50, 3),
    columns=["åˆ—A", "åˆ—B", "åˆ—C"]
)
# äº¤äº’å¼æ•°æ®è¡¨æ ¼ï¼ˆæ”¯æŒæ’åº/ç­›é€‰ï¼‰
st.dataframe(data, use_container_width=True)
# æ•°æ®ç»Ÿè®¡ä¿¡æ¯
st.write("ğŸ“Š æ•°æ®ç»Ÿè®¡ä¿¡æ¯ï¼š")
st.dataframe(data.describe(), use_container_width=True)

# 4. å†…ç½®äº¤äº’å¼å›¾è¡¨
st.subheader("ç»˜åˆ¶äº¤äº’å¼å›¾è¡¨")
# æŠ˜çº¿å›¾
st.line_chart(data, use_container_width=True)
# æ•£ç‚¹å›¾
st.scatter_chart(data, x="åˆ—A", y="åˆ—B", color="åˆ—C", use_container_width=True)

# 5. ä¾§è¾¹æ ï¼ˆå¸¸ç”¨ä½œç­›é€‰/å‚æ•°åŒºï¼‰
with st.sidebar:
    st.header("ä¾§è¾¹æ ç¤ºä¾‹")
    st.write("ä¾§è¾¹æ é€‚åˆæ”¾ç½®ç­›é€‰æ¡ä»¶ã€å‚æ•°é…ç½®ç­‰")
    show_data = st.checkbox("æ˜¾ç¤ºæ•°æ®è¡¨æ ¼", value=True)
    show_chart = st.checkbox("æ˜¾ç¤ºå›¾è¡¨", value=True)

# 6. è¿›åº¦æ¡æ¼”ç¤º
st.subheader("â³ è¿›åº¦æ¡æ¼”ç¤º")
progress_bar = st.progress(0)
status_text = st.empty()
for i in range(100):
    status_text.text(f"å½“å‰è¿›åº¦ï¼š{i+1}%")
    progress_bar.progress(i+1)
    # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œï¼ˆå®é™…åœºæ™¯å¯æ›¿æ¢ä¸ºæ•°æ®åŠ è½½/è®¡ç®—ï¼‰
    time.sleep(0.02)  # æš‚åœ0.02ç§’
status_text.text("è¿›åº¦å®Œæˆï¼âœ…")
```
